{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://databricks.com/blog/2019/12/05/processing-geospatial-data-at-scale-with-databricks.html\n",
    "# GEOPANDAS tutorial --> https://databricks.com/notebooks/geopandas-notebook.html\n",
    "# Shape Sao Paulo --> http://dados.prefeitura.sp.gov.br/pt_PT/dataset/referencia-urbana-do-municipio-de-sao-paulo\n",
    "# http://datageo.ambiente.sp.gov.br/coffey?_48_INSTANCE_KDzpt1cNV1RS_iframe_text=distrito+sao+paulo&enviar=Consultar&p_p_id=48_INSTANCE_KDzpt1cNV1RS&_48_INSTANCE_KDzpt1cNV1RS_iframe_avancado=false#_48_INSTANCE_KDzpt1cNV1RS_%3Dhttp%253A%252F%252Fdatageo.ambiente.sp.gov.br%252Fgeoportal%252Fcatalog%252Fsearch%252Fsearch.page%253Ftext%253Ddistrito%252520sao%252520paulo%2526avancado%253Dfalse\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark_conf = (SparkConf().set(\"spark.speculation\", \"false\"))\n",
    "sc = SparkContext.getOrCreate(conf = spark_conf)\n",
    "\n",
    "# sparkSession = spark --> in the case of EMR\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\",\"2\")\n",
    "\n",
    "sc.install_pypi_package(\"matplotlib\")\n",
    "sc.install_pypi_package(\"descartes\")\n",
    "sc.install_pypi_package(\"shapely\")\n",
    "sc.install_pypi_package(\"geopandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sp_shape = gpd.read_file('s3://mobility-traces-sp/aux-files/shape-sp/DISTRITO_MUNICIPAL_SP_SMDUPolygon.shp')\n",
    "\n",
    "def get_region(row, sp):\n",
    "    point = Point((float(row[0]), float(row[1])))\n",
    "    # 96 regions\n",
    "    for i in range(96):\n",
    "        if point.within(sp.loc[i, \"geometry\"]):\n",
    "            return sp.loc[i, \"Nome\"]\n",
    "    return \"None\"\n",
    "\n",
    "def get_region_udf(sp):\n",
    "    return udf(lambda x: get_region(x, sp))\n",
    "\n",
    "\n",
    "for day in range(1,32):   \n",
    "    traces = spark.read.parquet(\"s3a://mobility-traces-sp/processed-data/records-between-6-23-hour/MO_1510\"+str(day)+\"/\")\n",
    "    traces = traces.repartition(200)  \n",
    "    df_transformed = traces.withColumn(\"region\", get_region_udf(sc.broadcast(sp_shape).value)\n",
    "                                                           (struct(traces[\"longitude\"],\n",
    "                                                             traces[\"latitude\"])))\n",
    "    # difference between methods https://blog.knoldus.com/apache-spark-repartitioning-v-s-coalesce/\n",
    "    df_transformed.repartition(100).write.parquet(\"s3://mobility-traces-sp/processed-data/records_between-6-23-with-all-regions/MO_1510\"+str(day)+\"/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
