{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e1b880877c4f0d909dd2753a6a2e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>13</td><td>application_1588086860751_0014</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-86-127.ec2.internal:20888/proxy/application_1588086860751_0014/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-85-25.ec2.internal:8042/node/containerlogs/container_1588086860751_0014_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.2.1-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib64/python3.6/site-packages (from matplotlib) (1.14.5)\n",
      "Collecting python-dateutil>=2.1\n",
      "  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (88 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.13.0)\n",
      "Installing collected packages: pyparsing, python-dateutil, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.2.1 pyparsing-2.4.7 python-dateutil-2.8.1\n",
      "\n",
      "Collecting descartes\n",
      "  Using cached descartes-1.1.0-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: matplotlib in /mnt/tmp/1588127638972-0/lib64/python3.6/site-packages (from descartes) (3.2.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /mnt/tmp/1588127638972-0/lib/python3.6/site-packages (from matplotlib->descartes) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib64/python3.6/site-packages (from matplotlib->descartes) (1.14.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /mnt/tmp/1588127638972-0/lib/python3.6/site-packages (from matplotlib->descartes) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /mnt/tmp/1588127638972-0/lib64/python3.6/site-packages (from matplotlib->descartes) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/tmp/1588127638972-0/lib/python3.6/site-packages (from matplotlib->descartes) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib->descartes) (1.13.0)\n",
      "Installing collected packages: descartes\n",
      "Successfully installed descartes-1.1.0\n",
      "\n",
      "Collecting shapely\n",
      "  Using cached Shapely-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (1.8 MB)\n",
      "Installing collected packages: shapely\n",
      "Successfully installed shapely-1.7.0\n",
      "\n",
      "Collecting geopandas\n",
      "  Using cached geopandas-0.7.0-py2.py3-none-any.whl (928 kB)\n",
      "Collecting fiona\n",
      "  Using cached Fiona-1.8.13.post1-cp36-cp36m-manylinux1_x86_64.whl (14.7 MB)\n",
      "Collecting pandas>=0.23.0\n",
      "  Using cached pandas-1.0.3-cp36-cp36m-manylinux1_x86_64.whl (10.0 MB)\n",
      "Collecting pyproj>=2.2.0\n",
      "  Using cached pyproj-2.6.0-cp36-cp36m-manylinux2010_x86_64.whl (10.4 MB)\n",
      "Requirement already satisfied: shapely in /mnt/tmp/1588127638972-0/lib64/python3.6/site-packages (from geopandas) (1.7.0)\n",
      "Collecting click-plugins>=1.0\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting attrs>=17\n",
      "  Using cached attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.6/site-packages (from fiona->geopandas) (1.13.0)\n",
      "Collecting munch\n",
      "  Using cached munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting cligj>=0.5\n",
      "  Using cached cligj-0.5.0-py3-none-any.whl (5.7 kB)\n",
      "Collecting click<8,>=4.0\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /mnt/tmp/1588127638972-0/lib/python3.6/site-packages (from pandas>=0.23.0->geopandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.6/site-packages (from pandas>=0.23.0->geopandas) (1.14.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas>=0.23.0->geopandas) (2019.3)\n",
      "Installing collected packages: click, click-plugins, attrs, munch, cligj, fiona, pandas, pyproj, geopandas\n",
      "Successfully installed attrs-19.3.0 click-7.1.2 click-plugins-1.1.1 cligj-0.5.0 fiona-1.8.13.post1 geopandas-0.7.0 munch-2.5.0 pandas-1.0.3 pyproj-2.6.0"
     ]
    }
   ],
   "source": [
    "# https://databricks.com/blog/2019/12/05/processing-geospatial-data-at-scale-with-databricks.html\n",
    "# GEOPANDAS tutorial --> https://databricks.com/notebooks/geopandas-notebook.html\n",
    "# Shape Sao Paulo --> http://dados.prefeitura.sp.gov.br/pt_PT/dataset/referencia-urbana-do-municipio-de-sao-paulo\n",
    "# http://datageo.ambiente.sp.gov.br/coffey?_48_INSTANCE_KDzpt1cNV1RS_iframe_text=distrito+sao+paulo&enviar=Consultar&p_p_id=48_INSTANCE_KDzpt1cNV1RS&_48_INSTANCE_KDzpt1cNV1RS_iframe_avancado=false#_48_INSTANCE_KDzpt1cNV1RS_%3Dhttp%253A%252F%252Fdatageo.ambiente.sp.gov.br%252Fgeoportal%252Fcatalog%252Fsearch%252Fsearch.page%253Ftext%253Ddistrito%252520sao%252520paulo%2526avancado%253Dfalse\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark_conf = (SparkConf().set(\"spark.speculation\", \"false\"))\n",
    "sc = SparkContext.getOrCreate(conf = spark_conf)\n",
    "\n",
    "# sparkSession = spark --> in the case of EMR\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\",\"2\")\n",
    "\n",
    "sc.install_pypi_package(\"matplotlib\")\n",
    "sc.install_pypi_package(\"descartes\")\n",
    "sc.install_pypi_package(\"shapely\")\n",
    "sc.install_pypi_package(\"geopandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00776d7e4aba471c9bfb332c3dcfa79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"dt_server\", StringType()),\n",
    "    StructField(\"dt_avl\", StringType()),\n",
    "    StructField(\"line_id\", IntegerType()),\n",
    "    StructField(\"latitude\", DoubleType()),\n",
    "    StructField(\"longitude\", DoubleType()),\n",
    "    StructField(\"id_avl\", IntegerType()),\n",
    "    StructField(\"event\", IntegerType()),\n",
    "    StructField(\"id_point\", IntegerType()),\n",
    "    StructField(\"hour\", IntegerType()),\n",
    "    StructField(\"hour_diff\", FloatType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sp_shape = gpd.read_file('s3://mobility-traces-sp/aux-files/shape-sp/DISTRITO_MUNICIPAL_SP_SMDUPolygon.shp')\n",
    "\n",
    "def get_region(row, sp):\n",
    "    point = Point((float(row[0]), float(row[1])))\n",
    "    # 96 regions\n",
    "    for i in range(96):\n",
    "        if point.within(sp.loc[i, \"geometry\"]):\n",
    "            return sp.loc[i, \"Nome\"]\n",
    "    return \"None\"\n",
    "\n",
    "def get_region_udf(sp):\n",
    "    return udf(lambda x: get_region(x, sp))\n",
    "\n",
    "\n",
    "for day in range(1,32):   \n",
    "    traces = spark.read.parquet(\"s3a://mobility-traces-sp/processed-data/records-between-6-23-hour/MO_1510\"+str(day)+\"/\")\n",
    "    traces = traces.repartition(200)  \n",
    "    df_transformed = traces.withColumn(\"region\", get_region_udf(sc.broadcast(sp_shape).value)\n",
    "                                                           (struct(traces[\"longitude\"],\n",
    "                                                             traces[\"latitude\"])))\n",
    "    # difference between methods https://blog.knoldus.com/apache-spark-repartitioning-v-s-coalesce/\n",
    "    df_transformed.repartition(100).write.parquet(\"s3://mobility-traces-sp/processed-data/records_between-6-23-with-all-regions/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = spark.read.parquet(\"s3a://mobility-traces-sp/processed-data/records_between-6-23-with-all-regions/MO_15101/\")\n",
    "traces.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativas: select com presto/hive, nop\n",
    "# usar geomesa/geospark\n",
    "# usar geopandas de um jeito mais eficiente com UDF\n",
    "# dar join nas shapes com a tabela e ver qual bate nop\n",
    "#- problema spark nao tem suporte na tivo para geospatial data\n",
    "# - geospark\n",
    "# - geomesa\n",
    "# - magellan\n",
    "# # - databricks special type\n",
    "# - presto e airpal\n",
    "# - geopandas soznho nao da contar, tentar vetorizar como diz o link https://cloudarchitected.com/2019/07/geospatial-analytics-in-databricks-with-python-and-geomesa/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
