{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descrição\n",
    "# Após filtrar os dados mantendo os registros entre às 06:00 e 22:59 baseado no horario do servidor, \n",
    "# este notebook identifica a região da cidade de São Paulo \n",
    "# em que a localização do trace (lat/long de cada linha) foi registrada.\n",
    "# A cidade de São Paulo é divida em 96 distritos segundo Secretaria Municipal de Desenvolvimento Urbano de São Paulo.\n",
    "# \n",
    "#\n",
    "# Description\n",
    "# After filtering data keeping registers between 6:00 and 22:59 based in hour_server, \n",
    "# this notebook identifies the district/region of each\n",
    "# line/register based in fields lat/long and Sao Paulo districts' shape file.\n",
    "# Sao Paulo is a city splitted in 96 districts by Municipal Urban Planning and Licensing.\n",
    "#\n",
    "# Fontes de dados/ Data resources:\n",
    "# - Shapes/maps: http://geosampa.prefeitura.sp.gov.br/PaginasPublicas/_SBC.aspx\n",
    "# - Shapes: http://datageo.ambiente.sp.gov.br/coffey?_48_INSTANCE_KDzpt1cNV1RS_iframe_text=distrito+sao+paulo&enviar=Consultar&p_p_id=48_INSTANCE_KDzpt1cNV1RS&_48_INSTANCE_KDzpt1cNV1RS_iframe_avancado=false#_48_INSTANCE_KDzpt1cNV1RS_%3Dhttp%253A%252F%252Fdatageo.ambiente.sp.gov.br%252Fgeoportal%252Fcatalog%252Fsearch%252Fsearch.page%253Ftext%253Ddistrito%252520sao%252520paulo%2526avancado%253Dfalse\n",
    "# - Shapes: http://dados.prefeitura.sp.gov.br/pt_PT/dataset/referencia-urbana-do-municipio-de-sao-paulo\n",
    "# \n",
    "#\n",
    "# Tutoriais úteis/ useful tutorials:\n",
    "# https://databricks.com/blog/2019/12/05/processing-geospatial-data-at-scale-with-databricks.html\n",
    "# Geopandas - https://databricks.com/notebooks/geopandas-notebook.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Config\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark_conf = (SparkConf().set(\"spark.speculation\", \"false\"))\n",
    "sc = SparkContext.getOrCreate(conf = spark_conf)\n",
    "\n",
    "# spark = sparkSession\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\",\"2\")\n",
    "\n",
    "# installing required packages for this notebook session\n",
    "sc.install_pypi_package(\"matplotlib\")\n",
    "sc.install_pypi_package(\"descartes\")\n",
    "sc.install_pypi_package(\"shapely\")\n",
    "sc.install_pypi_package(\"geopandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Reading São Paulo ESRI shape file\n",
    "sp_shape = gpd.read_file('s3://mobility-traces-sp/aux-files/shape-sp/DISTRITO_MUNICIPAL_SP_SMDUPolygon.shp')\n",
    "\n",
    "# Receives a point (lat,long) in a row, and sp variable shape file\n",
    "def get_region(row, sp):\n",
    "    # row[0] = longitude, row[1] = latitude\n",
    "    point = Point((float(row[0]), float(row[1])))\n",
    "    \n",
    "    # 96 districts\n",
    "    for i in range(96):\n",
    "        # if the point is within that region, the function return the region Name\n",
    "        if point.within(sp.loc[i, \"geometry\"]):\n",
    "            return sp.loc[i, \"Nome\"]\n",
    "    # if the point is outside sp, the function returns \"None\"\n",
    "    return \"None\"\n",
    "\n",
    "# user definied function for spark\n",
    "def get_region_udf(sp):\n",
    "    return udf(lambda x: get_region(x, sp))\n",
    "\n",
    "# october day 1 to day 31\n",
    "for day in range(1,32):   \n",
    "    # reading file with records between 6:00 and 22:59\n",
    "    traces = spark.read.parquet(\"s3a://mobility-traces-sp/processed-data/using-server-hour/records-between-6-23-server-hour/MO_1510\"+str(day)+\"/\")\n",
    "    # reparting data in 200 partitions\n",
    "    traces = traces.repartition(200)  \n",
    "    \n",
    "    # adding a new column \"region\" in the traces dataframe\n",
    "    # the new column has the Sao Paulo district of the register lat/long\n",
    "    df_transformed = traces.withColumn(\"region\", get_region_udf(sc.broadcast(sp_shape).value)\n",
    "                                                           (struct(traces[\"longitude\"],\n",
    "                                                             traces[\"latitude\"])))\n",
    "    \n",
    "    # saves the dataframe data \n",
    "    df_transformed.repartition(100).write.parquet(\"s3://mobility-traces-sp/processed-data/using-server-hour/records_between-6-23-with-all-regions-server-hour/MO_1510\"+str(day)+\"/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
