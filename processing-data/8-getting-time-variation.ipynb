{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Time variation between consecutive records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support links\n",
    "# https://sparkbyexamples.com/spark/spark-difference-between-two-timestamps-in-seconds-minutes-and-hours/\n",
    "# https://intellipaat.com/community/10159/spark-add-new-column-to-dataframe-with-value-from-previous-row\n",
    "# https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-functions-windows.html#lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark_conf = (SparkConf().set(\"spark.speculation\", \"false\"))\n",
    "sc = SparkContext.getOrCreate(conf = spark_conf)\n",
    "\n",
    "# spark = sparkSession\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\",\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Selection possible traces/trajectory partitionBy id_avl and ordering by dt_avl\n",
    "window = Window.partitionBy('id_avl').orderBy('dt_avl')\n",
    "\n",
    "\n",
    "for day in range(1,32):\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/5-records-inside-sp-in-october/MO_1510\"+ str(day) +\"/\")  \n",
    "    \n",
    "    # calculating time variation for consecutive records \n",
    "    traces_time_variation = traces.select(\"*\", (F.to_timestamp('dt_avl').cast(LongType()) - F.to_timestamp(F.lag(\"dt_avl\").over(window)).cast(LongType())).alias(\"time_variation\"))\n",
    "    traces_time_variation.write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/6-records-inside-sp-in-october-with-time-variation/MO_1510\"+ str(day) +\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"boto3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting time variation statistics\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import boto3\n",
    "\n",
    "# new_line header\n",
    "csv_out = \"day,time_variation_mean,time_variation_min,time_variation_max,time_variation_stddev,time_variation_quantile_25,time_variation_quantile_50,time_variation_quantile_75,total_size,time_variation_higher_45s,time_variation_higher_100s\\n\"\n",
    "\n",
    "\n",
    "# from october 1 to october 31 \n",
    "for day in range(1,32):\n",
    "    \n",
    "    # reading files\n",
    "    traces_time_variation = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/6-records-inside-sp-in-october-with-time-variation/MO_1510\" + str(day) + \"/\")    \n",
    "    \n",
    "    total = traces_time_variation.count()\n",
    "    higher_45 = traces_time_variation.filter(\"time_variation > 45\").count()\n",
    "    higher_100 = traces_time_variation.filter(\"time_variation > 100\").count()\n",
    "    \n",
    "    # getting time_variation info (mean,min,max,stddev)\n",
    "    stats = traces_time_variation.agg(F.mean('time_variation').alias('mean'),\n",
    "                       F.min('time_variation').alias('min'),\n",
    "                       F.max('time_variation').alias('max'),\n",
    "                       F.stddev('time_variation').alias(\"stddev\")).collect()\n",
    "\n",
    "    # getting time_variation quantiles (25%, 50%, 75%)\n",
    "    # 0.0001 is the precision\n",
    "    time_variation_quantile = traces_time_variation.approxQuantile(\"time_variation\", [0.25,0.5,0.75], 0.0001)\n",
    "    \n",
    "    \n",
    "    csv_out += \"MO_1510\" + str(day) +\",\"+ str(stats[0][\"mean\"]) +\",\"+ str(stats[0][\"min\"]) + \",\" + str(stats[0][\"max\"]) + \",\" + str(stats[0][\"stddev\"]) + \",\" +str(time_variation_quantile[0]) + \",\" +str(time_variation_quantile[1]) +\",\" +str(time_variation_quantile[2]) + \",\" + str(total) +\",\"+ str(higher_45) + \",\"+ str(higher_100) + \"\\n\"\n",
    "\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# writing results in S3\n",
    "s3.put_object(Body=bytes(csv_out,\"utf-8\"), Bucket='mobility-traces-sp', Key='statistics/exploring-data/3-time-variation-stats.csv')    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
