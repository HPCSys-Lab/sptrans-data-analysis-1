{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descrição\n",
    "# Este script faz uma análise exploratório do dataset processado com os registros dentro da \n",
    "# área de são paulo e que estão entre 6 e 22:59:59. Analisa diferença entre as horas, e para cada quarters %.\n",
    "# O programa também conta quanto há de eventos 64 e 0 distintamente para cada dia, \n",
    "# quantas linhas e id_avl disitndos há para cada dia\n",
    "#\n",
    "# Description\n",
    "# This script analyzes the processed dataset (register in Sao Paulo region between 6-23 hours), \n",
    "# the hours diff distribution by quarters\n",
    "# Also counts the how many lines_id and id_avl for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Config\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark_conf = (SparkConf().set(\"spark.speculation\", \"false\"))\n",
    "sc = SparkContext.getOrCreate(conf = spark_conf)\n",
    "\n",
    "# spark = sparkSession\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\",\"2\")\n",
    "\n",
    "# installing necessary packages for notebook session\n",
    "sc.install_pypi_package(\"boto3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "\n",
    "# schema of the raw data\n",
    "custom_schema = StructType([\n",
    "    StructField(\"dt_server\", StringType()),\n",
    "    StructField(\"dt_avl\", StringType()),\n",
    "    StructField(\"line_id\", IntegerType()),\n",
    "    StructField(\"latitude\", DoubleType()),\n",
    "    StructField(\"longitude\", DoubleType()),\n",
    "    StructField(\"id_avl\", IntegerType()),\n",
    "    StructField(\"event\", IntegerType()),\n",
    "    StructField(\"id_point\", IntegerType()),\n",
    "    StructField(\"hour_server\", IntegerType()),\n",
    "    StructField(\"hour_avl\", IntegerType()),\n",
    "    StructField(\"hour_diff\", DoubleType())   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import boto3\n",
    "\n",
    "# new file header\n",
    "csv_out = \"day,line_id_distinct,id_avl_distinct,line-id_id-avl_distinct,event_0_count,event_64_count,id_point_distinct,hour_diff_mean,hour_diff_min,hour_diff_max,hour_diff_stddev,hour_diff_quantile_25,hour_diff,_quantile_50,hour_diff_quantile_75,hour_negative\\n\"\n",
    "\n",
    "# from october 1 to october 31 \n",
    "for day in range(1,32):\n",
    "\n",
    "    # reading files\n",
    "    traces = spark.read.schema(custom_schema).csv(\"s3://mobility-traces-sp/raw-hour-dt_server-dt_avl-hour_diff/MO_1510\" + str(day) + \"/\")\n",
    "    \n",
    "    # counting distinct line_id\n",
    "    line_id_distinct = traces.agg(F.countDistinct(\"line_id\")).collect()[0][0]\n",
    "\n",
    "    # counting distinct id_avl\n",
    "    id_avl_distinct = traces.agg(F.countDistinct(\"id_avl\")).collect()[0][0]\n",
    "\n",
    "    # counting distinct pairs line_id,id_avl\n",
    "    lines_avl_distinct = traces.select('line_id','id_avl').distinct().count()\n",
    "\n",
    "    # counting the numbers of registers with 0 in field \"event\"\n",
    "    number_0 = traces.filter(\"event == 0\").count()\n",
    "\n",
    "    # counting the numbers of registers with 64 in field \"event\"\n",
    "    number_64 = traces.filter(\"event == 64\").count()\n",
    "\n",
    "    # counting the numbers of distinct id_point\n",
    "    id_point_distinct = traces.agg(F.countDistinct(\"id_point\")).collect()[0][0]\n",
    "\n",
    "    # getting hour_diff info (mean,min,max,stddev)\n",
    "    hour_diff_info = traces.agg(F.mean('hour_diff').alias('mean'),\n",
    "                       F.min('hour_diff').alias('min'),\n",
    "                       F.max('hour_diff').alias('max'),\n",
    "                       F.stddev('hour_diff').alias(\"stddev\")).collect()\n",
    "    \n",
    "    hour_negative = traces.filter(\"hour_diff < 0\").count()\n",
    "\n",
    "    # getting hour_diff quantiles (25%, 50%, 75%)\n",
    "    # 0.0001 is the precision\n",
    "    hour_diff_quantile = traces.approxQuantile(\"hour_diff\", [0.25,0.5,0.75], 0.0001)\n",
    "\n",
    "    csv_out += \"MO_1510\" + str(day) +\",\"+ str(line_id_distinct) + \",\"  + str(id_avl_distinct)+ \",\" + str(lines_avl_distinct) + \",\" + str(number_0) + \",\" + str(number_64) + \",\" + str(id_point_distinct)+ \",\" + str(hour_diff_info[0][\"mean\"]) + \",\" + str(hour_diff_info[0][\"min\"]) + \",\" + str(hour_diff_info[0][\"max\"]) + \",\" + str(hour_diff_info[0][\"stddev\"]) + \",\" + str(hour_diff_quantile[0]) + \",\" + str(hour_diff_quantile[1]) + \",\" + str(hour_diff_quantile[2])+ \",\" + str(hour_negative) + \"\\n\"\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# writing results in S3\n",
    "s3.put_object(Body=bytes(csv_out,\"utf-8\"), Bucket='mobility-traces-sp', Key='statistics/exploring-data/1-distributions-hours-diff.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results study about dt_server, dt_avl, hour_diff,line_id,id_avl\n",
    "- There are at least 2700 different bus lines\n",
    "- There are more than 14600 unique avl equipements (id_avl)\n",
    "- The mean diference between the dt_server and dt_avl is 44s-60s\n",
    "- the min difference between dt_server and dt_avl is at least 5 minutes with the hour of dt_server being \n",
    "before the dt_avl\n",
    "- The max difference was around 21560 seconds (6 hours)\n",
    "- In the files the quartile 25% has hour diff less than 1.5 seconds\n",
    "- The 50% quartile had values between 1.9 - 2.6 seconds\n",
    "- The 75% quartile had values between 4 - 169 seconds\n",
    "- existem casos em que o relogio do servidor está com data menor que a data do avl (onibus), \n",
    "nao é a maior parte mas existem casos em que chega a milhoes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
