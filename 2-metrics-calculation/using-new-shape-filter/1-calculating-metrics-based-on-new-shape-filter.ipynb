{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b61dd7adb645ba945fe660fa537a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating time window, enriching file\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/8-map-matching/MO_1510{day}/\")\n",
    "    traces = traces.repartition(150)\n",
    "    df_enriched = traces\\\n",
    "        .withColumn('minute_avl',F.minute(F.col(\"dt_avl\")))\\\n",
    "        .withColumn(\"15_min_partition\",F.concat(F.col(\"hour_avl\"),F.lit(\"-\"),F.floor(F.col(\"minute_avl\")/15)))\\\n",
    "        .withColumn(\"30_min_partition\",F.concat(F.col(\"hour_avl\"),F.lit(\"-\"),F.floor(F.col(\"minute_avl\")/30)))\\\n",
    "        .drop(\"hour_diff\",\"time_variation\",\"trip_id\",\"direction\",\"route_id\",\"trip_head\")\n",
    "    df_enriched.repartition(150).write.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/8-map-matching-enriched/MO_1510{day}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f04569889b458cbe790185c8bece83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Counting shapes per interval\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "\n",
    "    traces = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/8-map-matching-enriched/MO_1510{day}/\")\n",
    "    \n",
    "    count_per_hour = traces.groupby(\"id_avl\",\"hour_avl\").agg(F.countDistinct(\"min_shape_sequence\").alias(\"count_shape\"))\n",
    "    count_per_15 = traces.groupby(\"id_avl\",\"15_min_partition\").agg(F.countDistinct(\"min_shape_sequence\").alias(\"count_shape\"))\n",
    "    count_per_30 = traces.groupby(\"id_avl\",\"30_min_partition\").agg(F.countDistinct(\"min_shape_sequence\").alias(\"count_shape\"))\n",
    "    \n",
    "    count_per_hour.write.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/statistic-count-shape/count-shape-per-hour/MO_1510{day}/\")\n",
    "    count_per_15.write.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/statistic-count-shape/count-shape-per-15/MO_1510{day}/\")\n",
    "    count_per_30.write.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/statistic-count-shape/count-shape-per-30/MO_1510{day}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279543b1e4f542d4b65c51e007ffb937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1\n",
      "[Row(mean=42.810012568077084, min=1, max=408, stddev=25.990349758208588)]\n",
      "[1.0, 2.0, 23.0, 48.0, 63.0]\n",
      "Day 4\n",
      "[Row(mean=21.882219865676916, min=1, max=308, stddev=28.00707575225608)]\n",
      "[1.0, 1.0, 1.0, 1.0, 46.0]\n",
      "Day 5\n",
      "[Row(mean=43.142443097325376, min=1, max=401, stddev=26.094426321579146)]\n",
      "[1.0, 2.0, 23.0, 49.0, 63.0]\n",
      "Day 12\n",
      "[Row(mean=22.44026323188304, min=1, max=394, stddev=28.419014746332355)]\n",
      "[1.0, 1.0, 1.0, 2.0, 47.0]\n",
      "Day 17\n",
      "[Row(mean=30.998816168488712, min=1, max=372, stddev=29.4613007206485)]\n",
      "[1.0, 1.0, 1.0, 28.0, 57.0]\n",
      "Day 20\n",
      "[Row(mean=42.80716431707881, min=1, max=395, stddev=26.18756897707706)]\n",
      "[1.0, 2.0, 22.0, 48.0, 63.0]"
     ]
    }
   ],
   "source": [
    "# Statistics about the aggregation and shape count\n",
    "\n",
    "# Count_per_hour\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "\n",
    "    counts_hour = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/statistic-count-shape/count-shape-per-hour/MO_1510{day}/\")\n",
    "\n",
    "    print(\"Day\", day)\n",
    "    stats = counts_hour.agg(F.mean('count_shape').alias('mean'),\n",
    "                           F.min('count_shape').alias('min'),\n",
    "                           F.max('count_shape').alias('max'),\n",
    "                           F.stddev('count_shape').alias(\"stddev\")).collect()\n",
    "\n",
    "    print(stats)\n",
    "\n",
    "    quantiles = counts_hour.approxQuantile(\"count_shape\", [0.0625,0.125,0.25,0.5,0.75], 0.0001)\n",
    "\n",
    "    print(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4c2bc55c254594acffd4ec42179bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1\n",
      "[Row(mean=11.637268480784863, min=1, max=118, stddev=7.877879289704797)]\n",
      "[1.0, 1.0, 3.0, 12.0, 18.0]\n",
      "Day 4\n",
      "[Row(mean=6.344709932441167, min=1, max=132, stddev=7.943601720018607)]\n",
      "[1.0, 1.0, 1.0, 1.0, 12.0]\n",
      "Day 5\n",
      "[Row(mean=11.746270213875848, min=1, max=142, stddev=7.927017643284481)]\n",
      "[1.0, 1.0, 3.0, 13.0, 18.0]\n",
      "Day 12\n",
      "[Row(mean=6.443837076035958, min=1, max=142, stddev=8.025781744439609)]\n",
      "[1.0, 1.0, 1.0, 1.0, 12.0]\n",
      "Day 17\n",
      "[Row(mean=8.601301699003407, min=1, max=138, stddev=8.444584560622992)]\n",
      "[1.0, 1.0, 1.0, 5.0, 17.0]\n",
      "Day 20\n",
      "[Row(mean=11.65199276794932, min=1, max=149, stddev=7.95421449029961)]\n",
      "[1.0, 1.0, 3.0, 13.0, 18.0]"
     ]
    }
   ],
   "source": [
    "# Statistics about the aggregation and shape count\n",
    "\n",
    "# Count_per_15\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "\n",
    "    counts_hour = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/statistic-count-shape/count-shape-per-15/MO_1510{day}/\")\n",
    "\n",
    "    print(\"Day\", day)\n",
    "    stats = counts_hour.agg(F.mean('count_shape').alias('mean'),\n",
    "                           F.min('count_shape').alias('min'),\n",
    "                           F.max('count_shape').alias('max'),\n",
    "                           F.stddev('count_shape').alias(\"stddev\")).collect()\n",
    "\n",
    "    print(stats)\n",
    "\n",
    "    quantiles = counts_hour.approxQuantile(\"count_shape\", [0.0625,0.125,0.25,0.5,0.75], 0.0001)\n",
    "\n",
    "    print(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976d7303f74d40c59f7114522c7e4f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1\n",
      "[Row(mean=22.364078578110384, min=1, max=222, stddev=14.60306275962971)]\n",
      "[1.0, 1.0, 9.0, 24.0, 34.0]\n",
      "Day 4\n",
      "[Row(mean=11.722527939964897, min=1, max=224, stddev=15.189746116741423)]\n",
      "[1.0, 1.0, 1.0, 1.0, 23.0]\n",
      "Day 5\n",
      "[Row(mean=22.561173273999273, min=1, max=231, stddev=14.684584105586737)]\n",
      "[1.0, 1.0, 9.0, 25.0, 35.0]\n",
      "Day 12\n",
      "[Row(mean=11.952658394480876, min=1, max=245, stddev=15.363135443429323)]\n",
      "[1.0, 1.0, 1.0, 1.0, 24.0]\n",
      "Day 17\n",
      "[Row(mean=16.300832285957387, min=1, max=237, stddev=16.059026342115963)]\n",
      "[1.0, 1.0, 1.0, 12.0, 31.0]\n",
      "Day 20\n",
      "[Row(mean=22.37836834650238, min=1, max=229, stddev=14.74188342400134)]\n",
      "[1.0, 1.0, 9.0, 25.0, 35.0]"
     ]
    }
   ],
   "source": [
    "# Statistics about the aggregation and shape count\n",
    "\n",
    "# Count_per_30\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "\n",
    "    counts_hour = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/statistic-count-shape/count-shape-per-30/MO_1510{day}/\")\n",
    "\n",
    "    print(\"Day\", day)\n",
    "    stats = counts_hour.agg(F.mean('count_shape').alias('mean'),\n",
    "                           F.min('count_shape').alias('min'),\n",
    "                           F.max('count_shape').alias('max'),\n",
    "                           F.stddev('count_shape').alias(\"stddev\")).collect()\n",
    "\n",
    "    print(stats)\n",
    "\n",
    "    quantiles = counts_hour.approxQuantile(\"count_shape\", [0.0625,0.125,0.25,0.5,0.75], 0.0001)\n",
    "\n",
    "    print(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fb329240bf41a69a682b9aa1187194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filtering data based on the number of shapes\n",
    "\n",
    "# filtering 1 hour\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    counts_hour   = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/statistic-count-shape/count-shape-per-hour/MO_1510{day}/\")\n",
    "    \n",
    "    counts_hour_to_exclude = counts_hour.filter(\"count_shape < 4\")\n",
    "    \n",
    "    traces_to_exclude = [f\"{row['id_avl']}-{row['hour_avl']}\" for row in counts_hour_to_exclude.collect()]\n",
    "    \n",
    "    traces = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/8-map-matching-enriched/MO_1510{day}/\")\n",
    "    \n",
    "    traces_filtered = traces.withColumn(\"combined_col\", f.concat(f.col(\"id_avl\"), f.lit(\"-\"), f.col(\"hour_avl\")))\\\n",
    "                        .where((f.col(\"combined_col\").isin(traces_to_exclude) == False) & (f.col(\"min_distance\") < 2100))\n",
    "    \n",
    "    traces_filtered.repartition(150).write.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/9-one-hour-filter/MO_1510{day}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a817f83e261444d85a32c99eb801236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filtering data based on the number of shapes\n",
    "\n",
    "# filtering 30 min\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "days_to_analyze = [12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    counts_30_min   = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/statistic-count-shape/count-shape-per-30/MO_1510{day}/\")\n",
    "        \n",
    "    counts_30_min_to_exclude = counts_30_min.filter(\"count_shape < 3\")\n",
    "    \n",
    "    traces_to_exclude = [f\"{row['id_avl']}-{row['30_min_partition']}\" for row in counts_30_min_to_exclude.collect()]\n",
    "    \n",
    "    traces = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/8-map-matching-enriched/MO_1510{day}/\")\n",
    "    traces = traces.repartition(\"30_min_partition\")\n",
    "    \n",
    "    traces_filtered = traces.withColumn(\"combined_col\", f.concat(f.col(\"id_avl\"), f.lit(\"-\"), f.col(\"30_min_partition\")))\\\n",
    "                        .where((f.col(\"combined_col\").isin(traces_to_exclude) == False) & (f.col(\"min_distance\") < 2100))\n",
    "    \n",
    "    traces_filtered.repartition(\"30_min_partition\").write.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/9-30-min-filter/MO_1510{day}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37ce9c2fa864620bf881964e8a1bd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1611244680370_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-89-235.ec2.internal:20888/proxy/application_1611244680370_0001/\" >Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-80-220.ec2.internal:8042/node/containerlogs/container_1611244680370_0001_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7411bc73a76c448fa1f28e05d8abe3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filtering data based on the number of shapes\n",
    "\n",
    "# filtering 15 min - count 1\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    counts_15_min = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/statistic-count-shape/count-shape-per-15/MO_1510{day}/\")\n",
    "        \n",
    "    counts_15_min_to_exclude = counts_15_min.filter(\"count_shape == 1\")\n",
    "    \n",
    "    traces_to_exclude = [f\"{row['id_avl']}-{row['15_min_partition']}\" for row in counts_15_min_to_exclude.collect()]\n",
    "    \n",
    "    traces = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/8-map-matching-enriched/MO_1510{day}/\")\n",
    "    \n",
    "    traces_filtered = traces.withColumn(\"combined_col\", f.concat(f.col(\"id_avl\"), f.lit(\"-\"), f.col(\"15_min_partition\")))\\\n",
    "                        .filter((f.col(\"combined_col\").isin(traces_to_exclude) == False) & (f.col(\"min_distance\") < 2100))\n",
    "    \n",
    "    traces_filtered.write.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/9-15-min-filter-count-1/MO_1510{day}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e532d4adbd94c21bcae99fffbfc8e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filtering data based on the number of shapes\n",
    "\n",
    "# filtering 15 min - count 1 or 2\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    counts_15_min = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/statistic-count-shape/count-shape-per-15/MO_1510{day}/\")\n",
    "        \n",
    "    counts_15_min_to_exclude = counts_15_min.filter(\"count_shape < 3\")\n",
    "    \n",
    "    traces_to_exclude = [f\"{row['id_avl']}-{row['15_min_partition']}\" for row in counts_15_min_to_exclude.collect()]\n",
    "    \n",
    "    traces = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/8-map-matching-enriched/MO_1510{day}/\")\n",
    "    \n",
    "    traces_filtered = traces.withColumn(\"combined_col\", f.concat(f.col(\"id_avl\"), f.lit(\"-\"), f.col(\"15_min_partition\")))\\\n",
    "                        .filter((f.col(\"combined_col\").isin(traces_to_exclude) == False) & (f.col(\"min_distance\") < 2100))\n",
    "    \n",
    "    traces_filtered.write.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/9-15-min-filter-count-2-or-1/MO_1510{day}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2e07a272b2492da3c67d36dfc2c1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>application_1611244680370_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-89-235.ec2.internal:20888/proxy/application_1611244680370_0007/\" >Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-95-49.ec2.internal:8042/node/containerlogs/container_1611244680370_0007_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11"
     ]
    }
   ],
   "source": [
    "11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959be4cfb084494cb8f191ca465657f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting haversine\n",
      "  Using cached https://files.pythonhosted.org/packages/f4/52/a13286844780c7b1740edbbee8a8f0524e2a6d51c068b59dda39a6a119f5/haversine-2.3.0-py2.py3-none-any.whl\n",
      "Installing collected packages: haversine\n",
      "Successfully installed haversine-2.3.0"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"haversine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d882c314294b3891500846f72ec9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating Speed for  1 hour\n",
    "from haversine import haversine, Unit\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# calculating speed for each register\n",
    "def calculate_speed(lon1,lat1,lon2,lat2,time_variation):\n",
    "    \n",
    "    # if lon2 and lat2 are available\n",
    "    if lon2 and lat2 and time_variation != 0:\n",
    "        coord_1 = float(lat1),float(lon1)\n",
    "        coord_2 = float(lat2),float(lon2)\n",
    "        distance = haversine(coord_1,coord_2,unit=Unit.METERS)\n",
    "        \n",
    "        # converting the speed from m/s to km/h multiplying by 3.6\n",
    "        return (distance/float(time_variation)) * 3.6\n",
    "    \n",
    "    # it there is no lat and long or time_variation = 0 \n",
    "    \n",
    "    else:\n",
    "        if time_variation == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "get_speed_udf = F.udf(calculate_speed, FloatType())\n",
    "\n",
    "window = Window.partitionBy(\"id_avl\",\"line_id\").orderBy('dt_avl') \n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    \n",
    "    # reading traces\n",
    "    traces = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/9-one-hour-filter/MO_1510{day}/\")\n",
    "    \n",
    "    traces = traces.withColumnRenamed(\"trace_x\", \"longitude\").withColumnRenamed(\"trace_y\", \"latitude\")\n",
    "    \n",
    "    # getting time variation\n",
    "    traces_time_variation = traces.select(\"*\", (F.to_timestamp('dt_avl').cast(LongType()) - F.to_timestamp(F.lag(\"dt_avl\").over(window)).cast(LongType())).alias(\"time_variation\"))\n",
    "    \n",
    "    # getting speed based on bus location\n",
    "    traces_speed_bus_location = traces_time_variation.select(\"*\", get_speed_udf(F.col(\"longitude\"),F.col(\"latitude\"),F.lag(F.col(\"longitude\")).over(window),F.lag(F.col(\"latitude\")).over(window),F.col(\"time_variation\")).alias(\"speed\"))\n",
    "    \n",
    "    traces_speed_bus_location.write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/10-one-hour-filter-speed-calculation/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792c5685bbaf486b80ffca1969399122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating Speed for 30 min \n",
    "from haversine import haversine, Unit\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# calculating speed for each register\n",
    "def calculate_speed(lon1,lat1,lon2,lat2,time_variation):\n",
    "    \n",
    "    # if lon2 and lat2 are available\n",
    "    if lon2 and lat2 and time_variation != 0:\n",
    "        coord_1 = float(lat1),float(lon1)\n",
    "        coord_2 = float(lat2),float(lon2)\n",
    "        distance = haversine(coord_1,coord_2,unit=Unit.METERS)\n",
    "        \n",
    "        # converting the speed from m/s to km/h multiplying by 3.6\n",
    "        return (distance/float(time_variation)) * 3.6\n",
    "    \n",
    "    # it there is no lat and long or time_variation = 0 \n",
    "    \n",
    "    else:\n",
    "        if time_variation == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "get_speed_udf = F.udf(calculate_speed, FloatType())\n",
    "\n",
    "window = Window.partitionBy(\"id_avl\",\"line_id\").orderBy('dt_avl') \n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    \n",
    "    # reading traces\n",
    "    traces = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/9-30-min-filter/MO_1510{day}/\")\n",
    "    \n",
    "    traces = traces.withColumnRenamed(\"trace_x\", \"longitude\").withColumnRenamed(\"trace_y\", \"latitude\")\n",
    "    \n",
    "    # getting time variation\n",
    "    traces_time_variation = traces.select(\"*\", (F.to_timestamp('dt_avl').cast(LongType()) - F.to_timestamp(F.lag(\"dt_avl\").over(window)).cast(LongType())).alias(\"time_variation\"))\n",
    "    \n",
    "    # getting speed based on bus location\n",
    "    traces_speed_bus_location = traces_time_variation.select(\"*\", get_speed_udf(F.col(\"longitude\"),F.col(\"latitude\"),F.lag(F.col(\"longitude\")).over(window),F.lag(F.col(\"latitude\")).over(window),F.col(\"time_variation\")).alias(\"speed\"))\n",
    "    \n",
    "    traces_speed_bus_location.write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/10-30-min-filter-speed-calculation/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26822d711384347b5017323c4e49421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating Speed 15 min - count 1 \n",
    "from haversine import haversine, Unit\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# calculating speed for each register\n",
    "def calculate_speed(lon1,lat1,lon2,lat2,time_variation):\n",
    "    \n",
    "    # if lon2 and lat2 are available\n",
    "    if lon2 and lat2 and time_variation != 0:\n",
    "        coord_1 = float(lat1),float(lon1)\n",
    "        coord_2 = float(lat2),float(lon2)\n",
    "        distance = haversine(coord_1,coord_2,unit=Unit.METERS)\n",
    "        \n",
    "        # converting the speed from m/s to km/h multiplying by 3.6\n",
    "        return (distance/float(time_variation)) * 3.6\n",
    "    \n",
    "    # it there is no lat and long or time_variation = 0 \n",
    "    \n",
    "    else:\n",
    "        if time_variation == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "get_speed_udf = F.udf(calculate_speed, FloatType())\n",
    "\n",
    "window = Window.partitionBy(\"id_avl\",\"line_id\").orderBy('dt_avl') \n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    \n",
    "    # reading traces\n",
    "    traces = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/9-15-min-filter-count-1/MO_1510{day}/\")\n",
    "    \n",
    "    traces = traces.withColumnRenamed(\"trace_x\", \"longitude\").withColumnRenamed(\"trace_y\", \"latitude\")\n",
    "    \n",
    "    # getting time variation\n",
    "    traces_time_variation = traces.select(\"*\", (F.to_timestamp('dt_avl').cast(LongType()) - F.to_timestamp(F.lag(\"dt_avl\").over(window)).cast(LongType())).alias(\"time_variation\"))\n",
    "    \n",
    "    # getting speed based on bus location\n",
    "    traces_speed_bus_location = traces_time_variation.select(\"*\", get_speed_udf(F.col(\"longitude\"),F.col(\"latitude\"),F.lag(F.col(\"longitude\")).over(window),F.lag(F.col(\"latitude\")).over(window),F.col(\"time_variation\")).alias(\"speed\"))\n",
    "    \n",
    "    traces_speed_bus_location.write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/10-15-min-count-1-filter-speed-calculation/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f1f8584f28416c8e04e5bbd082d85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating Speed 15 min - count 1 or 2\n",
    "from haversine import haversine, Unit\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# calculating speed for each register\n",
    "def calculate_speed(lon1,lat1,lon2,lat2,time_variation):\n",
    "    \n",
    "    # if lon2 and lat2 are available\n",
    "    if lon2 and lat2 and time_variation != 0:\n",
    "        coord_1 = float(lat1),float(lon1)\n",
    "        coord_2 = float(lat2),float(lon2)\n",
    "        distance = haversine(coord_1,coord_2,unit=Unit.METERS)\n",
    "        \n",
    "        # converting the speed from m/s to km/h multiplying by 3.6\n",
    "        return (distance/float(time_variation)) * 3.6\n",
    "    \n",
    "    # it there is no lat and long or time_variation = 0 \n",
    "    \n",
    "    else:\n",
    "        if time_variation == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "get_speed_udf = F.udf(calculate_speed, FloatType())\n",
    "\n",
    "window = Window.partitionBy(\"id_avl\",\"line_id\").orderBy('dt_avl') \n",
    "\n",
    "days_to_analyze = [4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    \n",
    "    # reading traces\n",
    "    traces = spark.read.parquet(f\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/9-15-min-filter-count-2-or-1/MO_1510{day}/\")\n",
    "    \n",
    "    traces = traces.withColumnRenamed(\"trace_x\", \"longitude\").withColumnRenamed(\"trace_y\", \"latitude\")\n",
    "    \n",
    "    # getting time variation\n",
    "    traces_time_variation = traces.select(\"*\", (F.to_timestamp('dt_avl').cast(LongType()) - F.to_timestamp(F.lag(\"dt_avl\").over(window)).cast(LongType())).alias(\"time_variation\"))\n",
    "    \n",
    "    # getting speed based on bus location\n",
    "    traces_speed_bus_location = traces_time_variation.select(\"*\", get_speed_udf(F.col(\"longitude\"),F.col(\"latitude\"),F.lag(F.col(\"longitude\")).over(window),F.lag(F.col(\"latitude\")).over(window),F.col(\"time_variation\")).alias(\"speed\"))\n",
    "    \n",
    "    traces_speed_bus_location.write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/10-15-min-count-2-or-1-filter-speed-calculation/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8e3a9d11234cba85c77692bbcdb76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Speed Filter for 1 hour filter\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    \n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/10-one-hour-filter-speed-calculation/MO_1510\"+str(day)+\"/\")    \n",
    "    traces_new = traces.filter(\"speed > 0.1\").filter(\"speed < 80\")\n",
    "    traces_new.write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-one-hour-filter/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcff3fb416f45e199d8b48546a7500a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Speed Filter for 30 min filter\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/10-30-min-filter-speed-calculation/MO_1510\"+str(day)+\"/\")    \n",
    "    traces_new = traces.filter(\"speed > 0.1\").filter(\"speed < 80\")\n",
    "    traces_new.write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-30-min-filter/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a212808c449403a8847eeaf0584cd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Speed Filter for 15 min filter 1 count\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/10-15-min-count-1-filter-speed-calculation/MO_1510\"+str(day)+\"/\")    \n",
    "    traces_new = traces.filter(\"speed > 0.1\").filter(\"speed < 80\")\n",
    "    traces_new.write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-15-min-filter-count-1/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f62731b61404fb590737698d9b41b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Speed Filter for 15 min filter 2 or 1 count\n",
    "\n",
    "days_to_analyze = [4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/10-15-min-count-2-or-1-filter-speed-calculation/MO_1510\"+str(day)+\"/\")    \n",
    "    traces_new = traces.filter(\"speed > 0.1\").filter(\"speed < 80\")\n",
    "    traces_new.write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-15-min-filter-count-2-or-1/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Degree Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76c5ebb781f40d895d334bfe6ea0bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating graph - One Hour\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-one-hour-filter/MO_1510\"+str(day)+\"/\")\n",
    "    df = traces.repartition(150)\n",
    "    \n",
    "    df_graph_id = df\\\n",
    "        .withColumn('minute_avl',F.minute(F.col(\"dt_avl\")))\\\n",
    "        .withColumn('graph_id',F.concat(F.col(\"hour_avl\"),F.lit(\"-\"),F.col(\"minute_avl\"),F.lit(\"-\"),F.col(\"region\")))\\\n",
    "        .drop(\"hour_diff\",\"dt_avl\",\"speed\")\n",
    "\n",
    "    df_graph_id.repartition(\"graph_id\").write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/12-traces-graph-id-one-hour-filter/MO_1510\"+str(day)+\"/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbae2f4ac62f4d1fa2fe8435a09abbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating graph - 30 min\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-30-min-filter/MO_1510\"+str(day)+\"/\")\n",
    "    df = traces.repartition(150)\n",
    "    \n",
    "    df_graph_id = df\\\n",
    "        .withColumn('minute_avl',F.minute(F.col(\"dt_avl\")))\\\n",
    "        .withColumn('graph_id',F.concat(F.col(\"hour_avl\"),F.lit(\"-\"),F.col(\"minute_avl\"),F.lit(\"-\"),F.col(\"region\")))\\\n",
    "        .drop(\"hour_diff\",\"dt_avl\",\"speed\")\n",
    "\n",
    "    df_graph_id.repartition(\"graph_id\").write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/12-traces-graph-id-30-min-filter/MO_1510\"+str(day)+\"/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e13cb137e24dd689f1fe1562727cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating graph - 15 min count 1\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-15-min-filter-count-1/MO_1510\"+str(day)+\"/\")\n",
    "    df = traces.repartition(150)\n",
    "    \n",
    "    df_graph_id = df\\\n",
    "        .withColumn('minute_avl',F.minute(F.col(\"dt_avl\")))\\\n",
    "        .withColumn('graph_id',F.concat(F.col(\"hour_avl\"),F.lit(\"-\"),F.col(\"minute_avl\"),F.lit(\"-\"),F.col(\"region\")))\\\n",
    "        .drop(\"hour_diff\",\"dt_avl\",\"speed\")\n",
    "\n",
    "    df_graph_id.repartition(\"graph_id\").write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/12-traces-graph-id-15-min-filter-count-1/MO_1510\"+str(day)+\"/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8e843a737046169e0a0aa17b10411a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating graph - 15 min count 1 or 2\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-15-min-filter-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    df = traces.repartition(150)\n",
    "    \n",
    "    df_graph_id = df\\\n",
    "        .withColumn('minute_avl',F.minute(F.col(\"dt_avl\")))\\\n",
    "        .withColumn('graph_id',F.concat(F.col(\"hour_avl\"),F.lit(\"-\"),F.col(\"minute_avl\"),F.lit(\"-\"),F.col(\"region\")))\\\n",
    "        .drop(\"hour_diff\",\"dt_avl\",\"speed\")\n",
    "\n",
    "    df_graph_id.repartition(\"graph_id\").write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/12-traces-graph-id-15-min-filter-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2529a4172424f0e8884f6578667a311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# joining the dataset - 1 hour filter\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/12-traces-graph-id-one-hour-filter/MO_1510\"+str(day)+\"/\")\n",
    "\n",
    "    df2 = df\n",
    "\n",
    "    df.alias('df1').join(df2.alias(\"df2\"),on=[\"graph_id\"],how=\"outer\")\\\n",
    "        .select(\n",
    "            f.col(\"df1.id_avl\").alias(\"id_avl_1\"),\n",
    "            f.col(\"df1.line_id\").alias(\"line_1\"),\n",
    "            f.col(\"df1.latitude\").alias(\"latitude_1\"),\n",
    "            f.col(\"df1.longitude\").alias(\"longitude_1\"),\n",
    "            f.col(\"df2.id_avl\").alias(\"id_avl_2\"),\n",
    "            f.col(\"df2.line_id\").alias(\"line_2\"),\n",
    "            f.col(\"df2.latitude\").alias(\"latitude_2\"),\n",
    "            f.col(\"df2.longitude\").alias(\"longitude_2\"),\n",
    "            f.col(\"df1.hour_avl\").alias(\"hour_avl\"),\n",
    "            f.col(\"df1.minute_avl\").alias(\"minute_avl\"),\n",
    "            f.col(\"df1.region\").alias(\"region\"),\n",
    "            f.col(\"graph_id\").alias(\"graph_id\"),\n",
    "\n",
    "    ).write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/13-joined-graph-one-hour/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b5c476b72c43149489f494ff3e5dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# joining the dataset - 30 min\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/12-traces-graph-id-30-min-filter/MO_1510\"+str(day)+\"/\")\n",
    "\n",
    "    df2 = df\n",
    "\n",
    "    df.alias('df1').join(df2.alias(\"df2\"),on=[\"graph_id\"],how=\"outer\")\\\n",
    "        .select(\n",
    "            f.col(\"df1.id_avl\").alias(\"id_avl_1\"),\n",
    "            f.col(\"df1.line_id\").alias(\"line_1\"),\n",
    "            f.col(\"df1.latitude\").alias(\"latitude_1\"),\n",
    "            f.col(\"df1.longitude\").alias(\"longitude_1\"),\n",
    "            f.col(\"df2.id_avl\").alias(\"id_avl_2\"),\n",
    "            f.col(\"df2.line_id\").alias(\"line_2\"),\n",
    "            f.col(\"df2.latitude\").alias(\"latitude_2\"),\n",
    "            f.col(\"df2.longitude\").alias(\"longitude_2\"),\n",
    "            f.col(\"df1.hour_avl\").alias(\"hour_avl\"),\n",
    "            f.col(\"df1.minute_avl\").alias(\"minute_avl\"),\n",
    "            f.col(\"df1.region\").alias(\"region\"),\n",
    "            f.col(\"graph_id\").alias(\"graph_id\"),\n",
    "\n",
    "    ).write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/13-joined-graph-30-min/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978c7631dfb44a5f81de900281fedbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# joining the dataset - 15min count 1\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/12-traces-graph-id-15-min-filter-count-1/MO_1510\"+str(day)+\"/\")\n",
    "\n",
    "    df2 = df\n",
    "\n",
    "    df.alias('df1').join(df2.alias(\"df2\"),on=[\"graph_id\"],how=\"outer\")\\\n",
    "        .select(\n",
    "            f.col(\"df1.id_avl\").alias(\"id_avl_1\"),\n",
    "            f.col(\"df1.line_id\").alias(\"line_1\"),\n",
    "            f.col(\"df1.latitude\").alias(\"latitude_1\"),\n",
    "            f.col(\"df1.longitude\").alias(\"longitude_1\"),\n",
    "            f.col(\"df2.id_avl\").alias(\"id_avl_2\"),\n",
    "            f.col(\"df2.line_id\").alias(\"line_2\"),\n",
    "            f.col(\"df2.latitude\").alias(\"latitude_2\"),\n",
    "            f.col(\"df2.longitude\").alias(\"longitude_2\"),\n",
    "            f.col(\"df1.hour_avl\").alias(\"hour_avl\"),\n",
    "            f.col(\"df1.minute_avl\").alias(\"minute_avl\"),\n",
    "            f.col(\"df1.region\").alias(\"region\"),\n",
    "            f.col(\"graph_id\").alias(\"graph_id\"),\n",
    "\n",
    "    ).write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/13-joined-graph-15-min-count-1/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212091f19c3d4e21aad1f67d8944fb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# joining the dataset - 15min count 2 or 1\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "days_to_analyze = [4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/12-traces-graph-id-15-min-filter-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "\n",
    "    df2 = df\n",
    "\n",
    "    df.alias('df1').join(df2.alias(\"df2\"),on=[\"graph_id\"],how=\"outer\")\\\n",
    "        .select(\n",
    "            f.col(\"df1.id_avl\").alias(\"id_avl_1\"),\n",
    "            f.col(\"df1.line_id\").alias(\"line_1\"),\n",
    "            f.col(\"df1.latitude\").alias(\"latitude_1\"),\n",
    "            f.col(\"df1.longitude\").alias(\"longitude_1\"),\n",
    "            f.col(\"df2.id_avl\").alias(\"id_avl_2\"),\n",
    "            f.col(\"df2.line_id\").alias(\"line_2\"),\n",
    "            f.col(\"df2.latitude\").alias(\"latitude_2\"),\n",
    "            f.col(\"df2.longitude\").alias(\"longitude_2\"),\n",
    "            f.col(\"df1.hour_avl\").alias(\"hour_avl\"),\n",
    "            f.col(\"df1.minute_avl\").alias(\"minute_avl\"),\n",
    "            f.col(\"df1.region\").alias(\"region\"),\n",
    "            f.col(\"graph_id\").alias(\"graph_id\"),\n",
    "\n",
    "    ).write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/13-joined-graph-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112487b8f10547d8909c4efe86b29bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eliminating duplicates - 1 Hour\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    joined = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/13-joined-graph-one-hour/MO_1510\"+str(day)+\"/\")\n",
    "    joined.repartition(\"graph_id\").filter(\"id_avl_1 != id_avl_2\").write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/14-no-duplicated-one-hour/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8182a9feb598473794dbafc75d95ce27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eliminating duplicates - 30min\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    joined = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/13-joined-graph-30-min/MO_1510\"+str(day)+\"/\")\n",
    "    joined.repartition(\"graph_id\").filter(\"id_avl_1 != id_avl_2\").write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/14-no-duplicated-30-min/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644c62fb1d374caca0d370f1956789da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eliminating duplicates - 15min count 1\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    joined = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/13-joined-graph-15-min-count-1/MO_1510\"+str(day)+\"/\")\n",
    "    joined.repartition(\"graph_id\").filter(\"id_avl_1 != id_avl_2\").write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/14-no-duplicated-15-min-count-1/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ec161f37894b4b8d239990aa98bad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eliminating duplicates - 15min count 2 or 1\n",
    "\n",
    "days_to_analyze = [4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    joined = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/13-joined-graph-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    joined.repartition(\"graph_id\").filter(\"id_avl_1 != id_avl_2\").write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/14-no-duplicated-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31024115d0d48c7b2f40bd9464b56bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sc.install_pypi_package(\"haversine\")\n",
    "\n",
    "from haversine import haversine, Unit\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def get_distance(lat1,lon1,lat2,lon2):\n",
    "    coord_1 = (lat1,lon1)\n",
    "    coord_2 = (lat2,lon2)\n",
    "\n",
    "    distance = haversine(coord_1,coord_2,unit=Unit.METERS)\n",
    "    \n",
    "    return distance\n",
    "    \n",
    "get_distance_udf = F.udf(get_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa984021c4b429ab6b756d9db75ce8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de30706040c40818b29f1de62d92288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa984021c4b429ab6b756d9db75ce8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating distances - 1 hour\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    no_repeated = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/14-no-duplicated-one-hour/MO_1510\"+str(day)+\"/\")\n",
    "    distance = no_repeated.withColumn(\"distance\",\n",
    "                    get_distance_udf(F.col(\"latitude_1\"),F.col(\"longitude_1\"),F.col(\"latitude_2\"),F.col(\"longitude_2\")))\n",
    "\n",
    "    df_final = distance.repartition(\"graph_id\")\n",
    "\n",
    "    df_final.filter(\"distance <= 100\")\\\n",
    "        .repartition(\"graph_id\")\\\n",
    "        .write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/15-distances-100m-one-hour/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4953d5a1828b488185ddcc81462001b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4953d5a1828b488185ddcc81462001b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ce2092bdf6466ba2bd394e85a8135b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating distances - 30 min\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    no_repeated = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/14-no-duplicated-30-min/MO_1510\"+str(day)+\"/\")\n",
    "    distance = no_repeated.withColumn(\"distance\",\n",
    "                    get_distance_udf(F.col(\"latitude_1\"),F.col(\"longitude_1\"),F.col(\"latitude_2\"),F.col(\"longitude_2\")))\n",
    "\n",
    "    df_final = distance.repartition(\"graph_id\")\n",
    "\n",
    "    df_final.filter(\"distance <= 100\")\\\n",
    "        .repartition(\"graph_id\")\\\n",
    "        .write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/15-distances-100m-30-min/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e7d6975a19430baa01e2113a3e2c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating distances - 15 min count 1\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    no_repeated = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/14-no-duplicated-15-min-count-1/MO_1510\"+str(day)+\"/\")\n",
    "    distance = no_repeated.withColumn(\"distance\",\n",
    "                    get_distance_udf(F.col(\"latitude_1\"),F.col(\"longitude_1\"),F.col(\"latitude_2\"),F.col(\"longitude_2\")))\n",
    "\n",
    "    df_final = distance.repartition(\"graph_id\")\n",
    "\n",
    "    df_final.filter(\"distance <= 100\")\\\n",
    "        .repartition(\"graph_id\")\\\n",
    "        .write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/15-distances-100m-15-min-count-1/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c036d7a2af4fcb9ea93cb2dd1ee79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating distances - 15 min count 2 or 1\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    no_repeated = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/14-no-duplicated-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    distance = no_repeated.withColumn(\"distance\",\n",
    "                    get_distance_udf(F.col(\"latitude_1\"),F.col(\"longitude_1\"),F.col(\"latitude_2\"),F.col(\"longitude_2\")))\n",
    "\n",
    "    df_final = distance.repartition(\"graph_id\")\n",
    "\n",
    "    df_final.filter(\"distance <= 100\")\\\n",
    "        .repartition(\"graph_id\")\\\n",
    "        .write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/15-distances-100m-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68c99db5ff24a19a2f1c370b9eb8767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68c99db5ff24a19a2f1c370b9eb8767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a147c85081e4fa49df637cba79800cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dropping duplicates in each graph - 1 hour\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    distances = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/15-distances-100m-one-hour/MO_1510\"+str(day)+\"/\")\n",
    "    df = distances.drop_duplicates(subset=[\"graph_id\",\"id_avl_1\",\"id_avl_2\"])\n",
    "    df.repartition(\"graph_id\").write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/16-no-repeated-contact-on-graph-one-hour/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae50737c15f4bb7bcc0f9169a65160f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae50737c15f4bb7bcc0f9169a65160f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82f7128b8334f7293861dddfd1519db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dropping duplicates in each graph - 30min \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    distances = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/15-distances-100m-30-min/MO_1510\"+str(day)+\"/\")\n",
    "    df = distances.drop_duplicates(subset=[\"graph_id\",\"id_avl_1\",\"id_avl_2\"])\n",
    "    df.repartition(\"graph_id\").write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/16-no-repeated-contact-on-graph-30-min/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6991e6295de14294af67d3c69d2a6f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dropping duplicates in each graph - 15min 1 count\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    distances = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/15-distances-100m-15-min-count-1/MO_1510\"+str(day)+\"/\")\n",
    "    df = distances.drop_duplicates(subset=[\"graph_id\",\"id_avl_1\",\"id_avl_2\"])\n",
    "    df.repartition(\"graph_id\").write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/16-no-repeated-contact-on-graph-15-min-count-1/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75666963ef154708878ccebc85d91b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dropping duplicates in each graph - 15min 1 or 2 count\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    distances = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/15-distances-100m-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    df = distances.drop_duplicates(subset=[\"graph_id\",\"id_avl_1\",\"id_avl_2\"])\n",
    "    df.repartition(\"graph_id\").write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/16-no-repeated-contact-on-graph-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dce26c7640246568ce1d2f3107e7831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dce26c7640246568ce1d2f3107e7831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fcdf490571f482484960b16b8fda9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Node Degree per vehicle per graph - 1 hour\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "for day in days_to_analyze:\n",
    "    \n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/16-no-repeated-contact-on-graph-one-hour/MO_1510\"+str(day)+\"/\")\n",
    "    df_counts = df.groupby(\"id_avl_1\",\"graph_id\").agg(F.countDistinct(\"id_avl_2\").alias(\"number_connections\"))\n",
    "\n",
    "    df_counts.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-per-vehicle-per-graph-one-hour/MO_110\"+str(day)+\"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2872d9d4b21e471bb9eaf596748a8421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2872d9d4b21e471bb9eaf596748a8421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d164820837b458f974e7d9ecbfea78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Node Degree per vehicle per graph - 30min\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "for day in days_to_analyze:\n",
    "    \n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/16-no-repeated-contact-on-graph-30-min/MO_1510\"+str(day)+\"/\")\n",
    "    df_counts = df.groupby(\"id_avl_1\",\"graph_id\").agg(F.countDistinct(\"id_avl_2\").alias(\"number_connections\"))\n",
    "\n",
    "    df_counts.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-per-vehicle-per-graph-30-min/MO_1510\"+str(day)+\"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c60bc5746c24dcaa6b4a590ead3ce9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Node Degree per vehicle per graph - 15min count 1\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "for day in days_to_analyze:\n",
    "    \n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/16-no-repeated-contact-on-graph-15-min-count-1/MO_1510\"+str(day)+\"/\")\n",
    "    df_counts = df.groupby(\"id_avl_1\",\"graph_id\").agg(F.countDistinct(\"id_avl_2\").alias(\"number_connections\"))\n",
    "\n",
    "    df_counts.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-per-vehicle-per-graph-15-min-count-1/MO_1510\"+str(day)+\"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7297e8e4113248b882ef9842e962d5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Node Degree per vehicle per graph - 15min count 2 or 1\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "for day in days_to_analyze:\n",
    "    \n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/16-no-repeated-contact-on-graph-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    df_counts = df.groupby(\"id_avl_1\",\"graph_id\").agg(F.countDistinct(\"id_avl_2\").alias(\"number_connections\"))\n",
    "\n",
    "    df_counts.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-per-vehicle-per-graph-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe82c4923394cb8b67e44eee04f34e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe82c4923394cb8b67e44eee04f34e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe82c4923394cb8b67e44eee04f34e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f72669960c4c9fa18f8d68e84488a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f72669960c4c9fa18f8d68e84488a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Node Degree avg per minute - 1 Hour\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "for day in days_to_analyze:\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-per-vehicle-per-graph-one-hour/MO_1510\"+str(day)+\"/\") \n",
    "\n",
    "    df = df.withColumn('splitted', F.split(df['graph_id'], '-'))\\\n",
    "        .withColumn('hour', F.col('splitted')[0])\\\n",
    "        .withColumn('minute', F.col('splitted')[1])\\\n",
    "        .withColumn('hour-minute', F.concat(F.col('hour'),F.lit(\":\"),F.col(\"minute\")))\\\n",
    "        .withColumn('region', F.col('splitted')[2])\\\n",
    "        .drop(\"splitted\")\n",
    "    \n",
    "    df = df.groupby(\"hour-minute\").agg(f.avg(\"number_connections\").alias(\"avg_degree\"))\\\n",
    "            .withColumn('time', F.date_format('hour-minute','HH:mm'))\\\n",
    "            .drop(\"hour-minute\")\n",
    "    \n",
    "    df.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-degree-per-minute-one-hour/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d79e26d591545e8b2ab941823f9559d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d79e26d591545e8b2ab941823f9559d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d79e26d591545e8b2ab941823f9559d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb0dfee460d4ffb8df06e07786ded54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb0dfee460d4ffb8df06e07786ded54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Node Degree avg per minute - 30min\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "for day in days_to_analyze:\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-per-vehicle-per-graph-30-min/MO_1510\"+str(day)+\"/\") \n",
    "\n",
    "    df = df.withColumn('splitted', F.split(df['graph_id'], '-'))\\\n",
    "        .withColumn('hour', F.col('splitted')[0])\\\n",
    "        .withColumn('minute', F.col('splitted')[1])\\\n",
    "        .withColumn('hour-minute', F.concat(F.col('hour'),F.lit(\":\"),F.col(\"minute\")))\\\n",
    "        .withColumn('region', F.col('splitted')[2])\\\n",
    "        .drop(\"splitted\")\n",
    "    \n",
    "    df = df.groupby(\"hour-minute\").agg(f.avg(\"number_connections\").alias(\"avg_degree\"))\\\n",
    "            .withColumn('time', F.date_format('hour-minute','HH:mm'))\\\n",
    "            .drop(\"hour-minute\")\n",
    "    \n",
    "    df.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-degree-per-minute-30-min/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfebdd6b3d74779baabf2a6d22f751a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Node Degree avg per minute - 15min count 1\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "for day in days_to_analyze:\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-per-vehicle-per-graph-15-min-count-1/MO_1510\"+str(day)+\"/\") \n",
    "\n",
    "    df = df.withColumn('splitted', F.split(df['graph_id'], '-'))\\\n",
    "        .withColumn('hour', F.col('splitted')[0])\\\n",
    "        .withColumn('minute', F.col('splitted')[1])\\\n",
    "        .withColumn('hour-minute', F.concat(F.col('hour'),F.lit(\":\"),F.col(\"minute\")))\\\n",
    "        .withColumn('region', F.col('splitted')[2])\\\n",
    "        .drop(\"splitted\")\n",
    "    \n",
    "    df = df.groupby(\"hour-minute\").agg(f.avg(\"number_connections\").alias(\"avg_degree\"))\\\n",
    "            .withColumn('time', F.date_format('hour-minute','HH:mm'))\\\n",
    "            .drop(\"hour-minute\")\n",
    "    \n",
    "    df.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-degree-per-minute-15-min-count-1/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb1ad55bd8b475483583426e50404dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Node Degree avg per minute - 15min count 2 or 1\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "days_to_analyze = [4,5,12,17,20]\n",
    "for day in days_to_analyze:\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-per-vehicle-per-graph-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\") \n",
    "\n",
    "    df = df.withColumn('splitted', F.split(df['graph_id'], '-'))\\\n",
    "        .withColumn('hour', F.col('splitted')[0])\\\n",
    "        .withColumn('minute', F.col('splitted')[1])\\\n",
    "        .withColumn('hour-minute', F.concat(F.col('hour'),F.lit(\":\"),F.col(\"minute\")))\\\n",
    "        .withColumn('region', F.col('splitted')[2])\\\n",
    "        .drop(\"splitted\")\n",
    "    \n",
    "    df = df.groupby(\"hour-minute\").agg(f.avg(\"number_connections\").alias(\"avg_degree\"))\\\n",
    "            .withColumn('time', F.date_format('hour-minute','HH:mm'))\\\n",
    "            .drop(\"hour-minute\")\n",
    "    \n",
    "    df.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-degree-per-minute-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d957faf72b4901ab0d656852ddf804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|5.644334654588108|7.044183651279824| 93|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[2.0, 3.0, 6.0]\n",
      "Day 5\n",
      "+-----------------+----------------+---+---+\n",
      "|              avg|          stddev|max|min|\n",
      "+-----------------+----------------+---+---+\n",
      "|5.806058398669762|7.38681447958697|105|  1|\n",
      "+-----------------+----------------+---+---+\n",
      "\n",
      "[2.0, 3.0, 7.0]\n",
      "Day 4\n",
      "+----------------+------------------+---+---+\n",
      "|             avg|            stddev|max|min|\n",
      "+----------------+------------------+---+---+\n",
      "|3.52670339046897|3.8160477658257674| 48|  1|\n",
      "+----------------+------------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 4.0]\n",
      "Day 12\n",
      "+-----------------+------------------+---+---+\n",
      "|              avg|            stddev|max|min|\n",
      "+-----------------+------------------+---+---+\n",
      "|3.610342046102568|3.9269995419072306| 51|  1|\n",
      "+-----------------+------------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 4.0]\n",
      "Day 17\n",
      "+------------------+-----------------+---+---+\n",
      "|               avg|           stddev|max|min|\n",
      "+------------------+-----------------+---+---+\n",
      "|4.3613470919987405|5.055531583618557| 64|  1|\n",
      "+------------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 3.0, 5.0]\n",
      "Day 20\n",
      "+---------------+-----------------+---+---+\n",
      "|            avg|           stddev|max|min|\n",
      "+---------------+-----------------+---+---+\n",
      "|5.6866493532945|7.132044743589377| 97|  1|\n",
      "+---------------+-----------------+---+---+\n",
      "\n",
      "[2.0, 3.0, 6.0]"
     ]
    }
   ],
   "source": [
    "# Degree vehicle per day statistics - 1 Hour\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "for day in days_to_analyze:\n",
    "    print(\"Day\",day)\n",
    "    count_connections = spark.read.parquet(f\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-per-vehicle-per-graph-one-hour/MO_1510{day}/\")\n",
    "\n",
    "    count_connections.agg(\n",
    "        F.avg(\"number_connections\").alias(\"avg\"),\n",
    "        F.stddev(\"number_connections\").alias(\"stddev\"),\n",
    "        F.max(\"number_connections\").alias(\"max\"),\n",
    "        F.min(\"number_connections\").alias(\"min\"),\n",
    "    ).show()\n",
    "\n",
    "    quantiles = count_connections.approxQuantile(\"number_connections\", [0.25,0.5,0.75], 0.0001)\n",
    "    print(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe8f74690e64676a2583bd3f88a639e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1\n",
      "+-----------------+----------------+---+---+\n",
      "|              avg|          stddev|max|min|\n",
      "+-----------------+----------------+---+---+\n",
      "|5.518458123628513|6.87792851269922| 93|  1|\n",
      "+-----------------+----------------+---+---+\n",
      "\n",
      "[2.0, 3.0, 6.0]\n",
      "Day 5\n",
      "+----------------+-----------------+---+---+\n",
      "|             avg|           stddev|max|min|\n",
      "+----------------+-----------------+---+---+\n",
      "|5.66541310853848|7.192349163388091|103|  1|\n",
      "+----------------+-----------------+---+---+\n",
      "\n",
      "[2.0, 3.0, 6.0]\n",
      "Day 4\n",
      "+------------------+-----------------+---+---+\n",
      "|               avg|           stddev|max|min|\n",
      "+------------------+-----------------+---+---+\n",
      "|3.4543202338590433|3.716322758827712| 47|  1|\n",
      "+------------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 4.0]\n",
      "Day 12\n",
      "+------------------+------------------+---+---+\n",
      "|               avg|            stddev|max|min|\n",
      "+------------------+------------------+---+---+\n",
      "|3.5378727073287246|3.8386312358068895| 51|  1|\n",
      "+------------------+------------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 4.0]\n",
      "Day 17\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.264431484268429|4.912882010709253| 63|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 20\n",
      "+-----------------+------------------+---+---+\n",
      "|              avg|            stddev|max|min|\n",
      "+-----------------+------------------+---+---+\n",
      "|5.553823123784149|6.9424050432163185| 94|  1|\n",
      "+-----------------+------------------+---+---+\n",
      "\n",
      "[2.0, 3.0, 6.0]"
     ]
    }
   ],
   "source": [
    "# Degree vehicle per day statistics - 30 min\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "for day in days_to_analyze:\n",
    "    print(\"Day\",day)\n",
    "    count_connections = spark.read.parquet(f\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-per-vehicle-per-graph-30-min/MO_1510{day}/\")\n",
    "\n",
    "    count_connections.agg(\n",
    "        F.avg(\"number_connections\").alias(\"avg\"),\n",
    "        F.stddev(\"number_connections\").alias(\"stddev\"),\n",
    "        F.max(\"number_connections\").alias(\"max\"),\n",
    "        F.min(\"number_connections\").alias(\"min\"),\n",
    "    ).show()\n",
    "\n",
    "    quantiles = count_connections.approxQuantile(\"number_connections\", [0.25,0.5,0.75], 0.0001)\n",
    "    print(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a2522624fb487cbb47bf6238ce7255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|5.487811100205848|6.777460010720816| 90|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[2.0, 3.0, 6.0]\n",
      "Day 5\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|5.632383800164916|7.074414975229834|100|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[2.0, 3.0, 6.0]\n",
      "Day 4\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|3.857202117572693|4.233486110324167| 48|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 12\n",
      "+------------------+------------------+---+---+\n",
      "|               avg|            stddev|max|min|\n",
      "+------------------+------------------+---+---+\n",
      "|3.8885813709132595|4.2064182854124095| 49|  1|\n",
      "+------------------+------------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 17\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.383824155196806|4.937071958196458| 61|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 3.0, 5.0]\n",
      "Day 20\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|5.530307716788036|6.840964116291219| 95|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[2.0, 3.0, 6.0]"
     ]
    }
   ],
   "source": [
    "# Degree vehicle per day statistics - 15min 1 count\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "for day in days_to_analyze:\n",
    "    print(\"Day\",day)\n",
    "    count_connections = spark.read.parquet(f\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-per-vehicle-per-graph-15-min-count-1/MO_1510{day}/\")\n",
    "\n",
    "    count_connections.agg(\n",
    "        F.avg(\"number_connections\").alias(\"avg\"),\n",
    "        F.stddev(\"number_connections\").alias(\"stddev\"),\n",
    "        F.max(\"number_connections\").alias(\"max\"),\n",
    "        F.min(\"number_connections\").alias(\"min\"),\n",
    "    ).show()\n",
    "\n",
    "    quantiles = count_connections.approxQuantile(\"number_connections\", [0.25,0.5,0.75], 0.0001)\n",
    "    print(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75092858f3d42e59bd838dc3c6b5516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|5.279602278902621|6.470637322630997| 88|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 3.0, 6.0]\n",
      "Day 4\n",
      "+-----------------+------------------+---+---+\n",
      "|              avg|            stddev|max|min|\n",
      "+-----------------+------------------+---+---+\n",
      "|3.261783514602363|3.4241518153877903| 42|  1|\n",
      "+-----------------+------------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 4.0]\n",
      "Day 5\n",
      "+------------------+-----------------+---+---+\n",
      "|               avg|           stddev|max|min|\n",
      "+------------------+-----------------+---+---+\n",
      "|5.4082539882365195|6.740563825682131| 96|  1|\n",
      "+------------------+-----------------+---+---+\n",
      "\n",
      "[2.0, 3.0, 6.0]\n",
      "Day 12\n",
      "+-----------------+------------------+---+---+\n",
      "|              avg|            stddev|max|min|\n",
      "+-----------------+------------------+---+---+\n",
      "|3.370718594517044|3.5893690944586036| 48|  1|\n",
      "+-----------------+------------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 4.0]\n",
      "Day 17\n",
      "+-----------------+------------------+---+---+\n",
      "|              avg|            stddev|max|min|\n",
      "+-----------------+------------------+---+---+\n",
      "|4.053031755789217|4.5626300581785975| 61|  1|\n",
      "+-----------------+------------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 20\n",
      "+-----------------+------------------+---+---+\n",
      "|              avg|            stddev|max|min|\n",
      "+-----------------+------------------+---+---+\n",
      "|5.311302211647513|6.5226419554501485| 91|  1|\n",
      "+-----------------+------------------+---+---+\n",
      "\n",
      "[1.0, 3.0, 6.0]"
     ]
    }
   ],
   "source": [
    "# Degree vehicle per day statistics - 15min 2 or 2 count\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "for day in days_to_analyze:\n",
    "    print(\"Day\",day)\n",
    "    count_connections = spark.read.parquet(f\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/connections-per-vehicle-per-graph-15-min-count-2-or-1/MO_1510{day}/\")\n",
    "\n",
    "    count_connections.agg(\n",
    "        F.avg(\"number_connections\").alias(\"avg\"),\n",
    "        F.stddev(\"number_connections\").alias(\"stddev\"),\n",
    "        F.max(\"number_connections\").alias(\"max\"),\n",
    "        F.min(\"number_connections\").alias(\"min\"),\n",
    "    ).show()\n",
    "\n",
    "    quantiles = count_connections.approxQuantile(\"number_connections\", [0.25,0.5,0.75], 0.0001)\n",
    "    print(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8fd965c1b484638b3f4b09c2918e160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adding connection_id - 1 Hour\n",
    "\n",
    "# Connection_id_column\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Creating a connection_id for each connection with the buses.\n",
    "# Connection_id = id_avl_1+id_avl_2 if id_avl_1 < id_avl_2\n",
    "# Connection_id = id_avl_2+id_avl_1 if id_avl_2 < id_avl_1\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/16-no-repeated-contact-on-graph-one-hour/MO_1510\"+str(day)+\"/\")\n",
    "\n",
    "    df = df.withColumn(\"connection_id\",\n",
    "        F.when(F.col(\"id_avl_1\") < F.col(\"id_avl_2\"), F.concat(F.col(\"id_avl_1\"),F.lit(\"-\"),F.col(\"id_avl_2\")))\n",
    "        .otherwise(F.concat(F.col(\"id_avl_2\"),F.lit(\"-\"),F.col(\"id_avl_1\"))))\n",
    "\n",
    "    df = df.repartition(\"graph_id\")\n",
    "\n",
    "\n",
    "\n",
    "    df.write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/17-no-repeated-contact-only-100-distances-with-connection-id-one-hour/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12590c693c7436b844cf44c3866116c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adding connection_id - 30min\n",
    "\n",
    "# Connection_id_column\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Creating a connection_id for each connection with the buses.\n",
    "# Connection_id = id_avl_1+id_avl_2 if id_avl_1 < id_avl_2\n",
    "# Connection_id = id_avl_2+id_avl_1 if id_avl_2 < id_avl_1\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/16-no-repeated-contact-on-graph-30-min/MO_1510\"+str(day)+\"/\")\n",
    "\n",
    "    df = df.withColumn(\"connection_id\",\n",
    "        F.when(F.col(\"id_avl_1\") < F.col(\"id_avl_2\"), F.concat(F.col(\"id_avl_1\"),F.lit(\"-\"),F.col(\"id_avl_2\")))\n",
    "        .otherwise(F.concat(F.col(\"id_avl_2\"),F.lit(\"-\"),F.col(\"id_avl_1\"))))\n",
    "\n",
    "    df = df.repartition(\"graph_id\")\n",
    "\n",
    "\n",
    "\n",
    "    df.write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/17-no-repeated-contact-only-100-distances-with-connection-id-30-min/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9777de3e4bf34ae6910d52a58c73fcf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adding connection_id - 15 min count 1\n",
    "\n",
    "# Connection_id_column\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Creating a connection_id for each connection with the buses.\n",
    "# Connection_id = id_avl_1+id_avl_2 if id_avl_1 < id_avl_2\n",
    "# Connection_id = id_avl_2+id_avl_1 if id_avl_2 < id_avl_1\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/16-no-repeated-contact-on-graph-15-min-count-1/MO_1510\"+str(day)+\"/\")\n",
    "\n",
    "    df = df.withColumn(\"connection_id\",\n",
    "        F.when(F.col(\"id_avl_1\") < F.col(\"id_avl_2\"), F.concat(F.col(\"id_avl_1\"),F.lit(\"-\"),F.col(\"id_avl_2\")))\n",
    "        .otherwise(F.concat(F.col(\"id_avl_2\"),F.lit(\"-\"),F.col(\"id_avl_1\"))))\n",
    "\n",
    "    df = df.repartition(\"graph_id\")\n",
    "\n",
    "\n",
    "\n",
    "    df.write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/17-no-repeated-contact-only-100-distances-with-connection-id-15-min-count-1/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b8d0090add4a25a19d827c44245e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Adding connection_id - 15 min count 2 or 1\n",
    "\n",
    "# Connection_id_column\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Creating a connection_id for each connection with the buses.\n",
    "# Connection_id = id_avl_1+id_avl_2 if id_avl_1 < id_avl_2\n",
    "# Connection_id = id_avl_2+id_avl_1 if id_avl_2 < id_avl_1\n",
    "\n",
    "days_to_analyze = [4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/16-no-repeated-contact-on-graph-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "\n",
    "    df = df.withColumn(\"connection_id\",\n",
    "        F.when(F.col(\"id_avl_1\") < F.col(\"id_avl_2\"), F.concat(F.col(\"id_avl_1\"),F.lit(\"-\"),F.col(\"id_avl_2\")))\n",
    "        .otherwise(F.concat(F.col(\"id_avl_2\"),F.lit(\"-\"),F.col(\"id_avl_1\"))))\n",
    "\n",
    "    df = df.repartition(\"graph_id\")\n",
    "\n",
    "\n",
    "\n",
    "    df.write.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/17-no-repeated-contact-only-100-distances-with-connection-id-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d35ba07446f43dc8cc990d7b17c544c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving count of repeated connections - 1 hour\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/17-no-repeated-contact-only-100-distances-with-connection-id-one-hour/MO_1510\"+str(day)+\"/\")\n",
    "    df = df.drop_duplicates(subset=[\"connection_id\",\"graph_id\"])\n",
    "\n",
    "    df_counts = df.groupby(\"connection_id\").agg(F.count(\"connection_id\").alias(\"count_per_day\"))\n",
    "\n",
    "    df_counts.write.parquet(f\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/repeated-connection-per-day-one-hour/MO_1510{day}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c5c20d300a46f19f2ff6f7cd849921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving count of repeated connections - 30 min\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/17-no-repeated-contact-only-100-distances-with-connection-id-30-min/MO_1510\"+str(day)+\"/\")\n",
    "    df = df.drop_duplicates(subset=[\"connection_id\",\"graph_id\"])\n",
    "\n",
    "    df_counts = df.groupby(\"connection_id\").agg(F.count(\"connection_id\").alias(\"count_per_day\"))\n",
    "\n",
    "    df_counts.write.parquet(f\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/repeated-connection-per-day-30-min/MO_1510{day}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901f242585874b2283c11738db04666d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving count of repeated connections - 15 min count 1\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/17-no-repeated-contact-only-100-distances-with-connection-id-15-min-count-1/MO_1510\"+str(day)+\"/\")\n",
    "    df = df.drop_duplicates(subset=[\"connection_id\",\"graph_id\"])\n",
    "\n",
    "    df_counts = df.groupby(\"connection_id\").agg(F.count(\"connection_id\").alias(\"count_per_day\"))\n",
    "\n",
    "    df_counts.write.parquet(f\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/repeated-connection-per-day-15-min-count-1/MO_1510{day}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b113e7fe38b43d58686ba48c61e25ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving count of repeated connections - 15 min count 1 or 2\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/17-no-repeated-contact-only-100-distances-with-connection-id-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    df = df.drop_duplicates(subset=[\"connection_id\",\"graph_id\"])\n",
    "\n",
    "    df_counts = df.groupby(\"connection_id\").agg(F.count(\"connection_id\").alias(\"count_per_day\"))\n",
    "\n",
    "    df_counts.write.parquet(f\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/repeated-connection-per-day-15-min-count-2-or-1/MO_1510{day}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d0113955db451fb8c0fef5844ec61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1\n",
      "+-----------------+------------------+---+---+\n",
      "|              avg|            stddev|max|min|\n",
      "+-----------------+------------------+---+---+\n",
      "|4.418265384641201|7.0238853047239616|332|  1|\n",
      "+-----------------+------------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 5\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.543664223346176|7.218399856270012|295|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 4\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.807141569349781|8.300607153912106|422|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 12\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.835611449805895|8.361017864475857|357|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 17\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.598482532554776|7.640259517463284|286|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 20\n",
      "+------------------+-----------------+---+---+\n",
      "|               avg|           stddev|max|min|\n",
      "+------------------+-----------------+---+---+\n",
      "|4.4551404117507225|7.015925516603221|332|  1|\n",
      "+------------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of repeated connections per day/duration of the connections - 1 Hour\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    print(\"Day\",day)\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/17-no-repeated-contact-only-100-distances-with-connection-id-one-hour/MO_1510\"+str(day)+\"/\")\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"connection_id\",\"graph_id\"])\n",
    "\n",
    "    df_counts = df.groupby(\"connection_id\").agg(F.count(\"connection_id\").alias(\"count_per_day\"))\n",
    "\n",
    "    df_counts.agg(\n",
    "        F.avg(\"count_per_day\").alias(\"avg\"),\n",
    "        F.stddev(\"count_per_day\").alias(\"stddev\"),\n",
    "        F.max(\"count_per_day\").alias(\"max\"),\n",
    "        F.min(\"count_per_day\").alias(\"min\"),\n",
    "    ).show()\n",
    "\n",
    "    quantiles = df_counts.approxQuantile(\"count_per_day\", [0.25,0.5,0.75], 0.0001)\n",
    "    print(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa78e7cd1c044a42bbd8631891720f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.258900781473971|6.551075751418052|309|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 5\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.368512436840792|6.720363735857488|496|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 4\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.601371511788974|7.681885898116999|762|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 12\n",
      "+-----------------+----------------+---+---+\n",
      "|              avg|          stddev|max|min|\n",
      "+-----------------+----------------+---+---+\n",
      "|4.630285338340951|7.56981262289641|608|  1|\n",
      "+-----------------+----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 17\n",
      "+----------------+-----------------+---+---+\n",
      "|             avg|           stddev|max|min|\n",
      "+----------------+-----------------+---+---+\n",
      "|4.41868486263429|7.064170287112986|387|  1|\n",
      "+----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 20\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.291492334595094|6.555192771559289|324|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of repeated connections per day/duration of the connections - 30 min\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    print(\"Day\",day)\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/17-no-repeated-contact-only-100-distances-with-connection-id-30-min/MO_1510\"+str(day)+\"/\")\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"connection_id\",\"graph_id\"])\n",
    "\n",
    "    df_counts = df.groupby(\"connection_id\").agg(F.count(\"connection_id\").alias(\"count_per_day\"))\n",
    "\n",
    "    df_counts.agg(\n",
    "        F.avg(\"count_per_day\").alias(\"avg\"),\n",
    "        F.stddev(\"count_per_day\").alias(\"stddev\"),\n",
    "        F.max(\"count_per_day\").alias(\"max\"),\n",
    "        F.min(\"count_per_day\").alias(\"min\"),\n",
    "    ).show()\n",
    "\n",
    "    quantiles = df_counts.approxQuantile(\"count_per_day\", [0.25,0.5,0.75], 0.0001)\n",
    "    print(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b871232258546c3951ae61f296a086f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.225720488260441|6.535584229258312|601|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 5\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.333370641233261|6.767551791007388|703|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 4\n",
      "+------------------+------------------+---+---+\n",
      "|               avg|            stddev|max|min|\n",
      "+------------------+------------------+---+---+\n",
      "|5.2916404328334075|16.566923423100345|946|  1|\n",
      "+------------------+------------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 12\n",
      "+-----------------+------------------+---+---+\n",
      "|              avg|            stddev|max|min|\n",
      "+-----------------+------------------+---+---+\n",
      "|5.231138740945351|15.378434005437626|942|  1|\n",
      "+-----------------+------------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 17\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.562765726159686|9.361071125879848|977|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 20\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.263400571746211|6.635971289256592|751|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of repeated connections per day/duration of the connections - 15 min count 1\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    print(\"Day\",day)\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/17-no-repeated-contact-only-100-distances-with-connection-id-15-min-count-1/MO_1510\"+str(day)+\"/\")\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"connection_id\",\"graph_id\"])\n",
    "\n",
    "    df_counts = df.groupby(\"connection_id\").agg(F.count(\"connection_id\").alias(\"count_per_day\"))\n",
    "\n",
    "    df_counts.agg(\n",
    "        F.avg(\"count_per_day\").alias(\"avg\"),\n",
    "        F.stddev(\"count_per_day\").alias(\"stddev\"),\n",
    "        F.max(\"count_per_day\").alias(\"max\"),\n",
    "        F.min(\"count_per_day\").alias(\"min\"),\n",
    "    ).show()\n",
    "\n",
    "    quantiles = df_counts.approxQuantile(\"count_per_day\", [0.25,0.5,0.75], 0.0001)\n",
    "    print(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1768ab8472449138e1eb9723463b0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|3.957072955793974|5.735315427907568|260|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 4.0]\n",
      "Day 4\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.084992890713911|5.900434384572302|433|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 5\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.047415762469089|5.871192171021094|286|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 4.0]\n",
      "Day 12\n",
      "+-----------------+-----------------+---+---+\n",
      "|              avg|           stddev|max|min|\n",
      "+-----------------+-----------------+---+---+\n",
      "|4.150720279651191|5.974793123506562|321|  1|\n",
      "+-----------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 5.0]\n",
      "Day 17\n",
      "+------------------+-----------------+---+---+\n",
      "|               avg|           stddev|max|min|\n",
      "+------------------+-----------------+---+---+\n",
      "|4.0171958835039066|5.812794557988134|285|  1|\n",
      "+------------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 4.0]\n",
      "Day 20\n",
      "+------------------+-----------------+---+---+\n",
      "|               avg|           stddev|max|min|\n",
      "+------------------+-----------------+---+---+\n",
      "|3.9817637556751206|5.739456702450218|281|  1|\n",
      "+------------------+-----------------+---+---+\n",
      "\n",
      "[1.0, 2.0, 4.0]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of repeated connections per day/duration of the connections - 15 min count 2 or 1\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    print(\"Day\",day)\n",
    "    df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/17-no-repeated-contact-only-100-distances-with-connection-id-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"connection_id\",\"graph_id\"])\n",
    "\n",
    "    df_counts = df.groupby(\"connection_id\").agg(F.count(\"connection_id\").alias(\"count_per_day\"))\n",
    "\n",
    "    df_counts.agg(\n",
    "        F.avg(\"count_per_day\").alias(\"avg\"),\n",
    "        F.stddev(\"count_per_day\").alias(\"stddev\"),\n",
    "        F.max(\"count_per_day\").alias(\"max\"),\n",
    "        F.min(\"count_per_day\").alias(\"min\"),\n",
    "    ).show()\n",
    "\n",
    "    quantiles = df_counts.approxQuantile(\"count_per_day\", [0.25,0.5,0.75], 0.0001)\n",
    "    print(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e0e4c5639841a198d385e002f754c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_counts = spark.read.parquet(f\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/connectivity-metrics/repeated-connection-per-day-one-hour/MO_15101/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52285cee54034bcfa7104d63858cf978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|connection_id|count_per_day|\n",
      "+-------------+-------------+\n",
      "|  52622-57429|          332|\n",
      "+-------------+-------------+"
     ]
    }
   ],
   "source": [
    "df_counts.filter(\"count_per_day == 332\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6ab0e89ede42e7bae81ed66a088a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-one-hour-filter/MO_15101\")\n",
    "df1 = df.filter(\"id_avl == '52622'\").sort(\"dt_avl\").repartition(1).write.csv(\"s3://mobility-traces-sp/bus-52622-2/\",header=True)\n",
    "df2 = df.filter(\"id_avl == '57429'\").sort(\"dt_avl\").repartition(1).write.csv(\"s3://mobility-traces-sp/bus-57429-2/\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dac01e7b7454f30b5445f0130f4133c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1\n",
      "+--------------------+\n",
      "|number_distinct_conn|\n",
      "+--------------------+\n",
      "|             4916283|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+\n",
      "|total_per_day|\n",
      "+-------------+\n",
      "|     21721443|\n",
      "+-------------+\n",
      "\n",
      "Day 5\n",
      "+--------------------+\n",
      "|number_distinct_conn|\n",
      "+--------------------+\n",
      "|             4964419|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+\n",
      "|total_per_day|\n",
      "+-------------+\n",
      "|     22556653|\n",
      "+-------------+\n",
      "\n",
      "Day 4\n",
      "+--------------------+\n",
      "|number_distinct_conn|\n",
      "+--------------------+\n",
      "|             1076038|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+\n",
      "|total_per_day|\n",
      "+-------------+\n",
      "|      5172667|\n",
      "+-------------+\n",
      "\n",
      "Day 12\n",
      "+--------------------+\n",
      "|number_distinct_conn|\n",
      "+--------------------+\n",
      "|             1156072|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+\n",
      "|total_per_day|\n",
      "+-------------+\n",
      "|      5590315|\n",
      "+-------------+\n",
      "\n",
      "Day 17\n",
      "+--------------------+\n",
      "|number_distinct_conn|\n",
      "+--------------------+\n",
      "|             2335602|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+\n",
      "|total_per_day|\n",
      "+-------------+\n",
      "|     10740225|\n",
      "+-------------+\n",
      "\n",
      "Day 20\n",
      "+--------------------+\n",
      "|number_distinct_conn|\n",
      "+--------------------+\n",
      "|             4913264|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+\n",
      "|total_per_day|\n",
      "+-------------+\n",
      "|     21889281|\n",
      "+-------------+"
     ]
    }
   ],
   "source": [
    "# Total number of distinct connections per day for - 1 hour\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    print(\"Day\",day)\n",
    "    connections = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/17-no-repeated-contact-only-100-distances-with-connection-id-one-hour/MO_1510\"+str(day)+\"/\")\n",
    "\n",
    "    connections.agg(F.countDistinct(\"connection_id\").alias(\"number_distinct_conn\")).show()\n",
    "    connections.groupby(\"graph_id\",\"connection_id\").agg(F.countDistinct(\"connection_id\").alias(\"count_novo\"))\\\n",
    "        .agg(F.sum(\"count_novo\").alias(\"total_per_day\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5b824a03a7485c82c1761c0a27d9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1\n",
      "+--------------------+\n",
      "|number_distinct_conn|\n",
      "+--------------------+\n",
      "|             4756908|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+\n",
      "|total_per_day|\n",
      "+-------------+\n",
      "|     18823432|\n",
      "+-------------+\n",
      "\n",
      "Day 5\n",
      "+--------------------+\n",
      "|number_distinct_conn|\n",
      "+--------------------+\n",
      "|             4797624|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+\n",
      "|total_per_day|\n",
      "+-------------+\n",
      "|     19417979|\n",
      "+-------------+\n",
      "\n",
      "Day 4\n",
      "+--------------------+\n",
      "|number_distinct_conn|\n",
      "+--------------------+\n",
      "|             1035969|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+\n",
      "|total_per_day|\n",
      "+-------------+\n",
      "|      4231926|\n",
      "+-------------+\n",
      "\n",
      "Day 12\n",
      "+--------------------+\n",
      "|number_distinct_conn|\n",
      "+--------------------+\n",
      "|             1113387|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+\n",
      "|total_per_day|\n",
      "+-------------+\n",
      "|      4621358|\n",
      "+-------------+\n",
      "\n",
      "Day 17\n",
      "+--------------------+\n",
      "|number_distinct_conn|\n",
      "+--------------------+\n",
      "|             2251818|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+\n",
      "|total_per_day|\n",
      "+-------------+\n",
      "|      9045994|\n",
      "+-------------+\n",
      "\n",
      "Day 20\n",
      "+--------------------+\n",
      "|number_distinct_conn|\n",
      "+--------------------+\n",
      "|             4745714|\n",
      "+--------------------+\n",
      "\n",
      "+-------------+\n",
      "|total_per_day|\n",
      "+-------------+\n",
      "|     18896312|\n",
      "+-------------+"
     ]
    }
   ],
   "source": [
    "# Total number of distinct connections per day for - 15 min count 1 or 2\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "days_to_analyze = [1,5,4,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    print(\"Day\",day)\n",
    "    connections = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/17-no-repeated-contact-only-100-distances-with-connection-id-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "\n",
    "    connections.agg(F.countDistinct(\"connection_id\").alias(\"number_distinct_conn\")).show()\n",
    "    connections.groupby(\"graph_id\",\"connection_id\").agg(F.countDistinct(\"connection_id\").alias(\"count_novo\"))\\\n",
    "        .agg(F.sum(\"count_novo\").alias(\"total_per_day\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bus Active Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d801f5c526374d89aa16380af16fe96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/8f/6308b1f14b492369363066367401312370cc16719f29f3fed45e6a972e41/boto3-1.16.58-py2.py3-none-any.whl (130kB)\n",
      "Collecting botocore<1.20.0,>=1.19.58 (from boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/1a/22/e423cfaed6f89de9bfc3978d02db7284832d658b0c0695bdce9aceb1fc05/botocore-1.19.58-py2.py3-none-any.whl (7.2MB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.20.0,>=1.19.58->boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "Collecting urllib3<1.27,>=1.25.4; python_version != \"3.4\" (from botocore<1.20.0,>=1.19.58->boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl (136kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.58->boto3)\n",
      "Installing collected packages: python-dateutil, urllib3, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.16.58 botocore-1.19.58 python-dateutil-2.8.1 s3transfer-0.3.4 urllib3-1.26.2"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"boto3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total vehicles per day - 1 Hour\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import boto3\n",
    "\n",
    "csv_out = \"day,number_of_vehicles\\n\"\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-one-hour-filter/MO_1510\"+str(day)+\"/\")\n",
    "    n_vehicles_day = traces.select(F.countDistinct(\"id_avl\").alias(\"count\")).collect()[0][\"count\"]\n",
    "    \n",
    "    csv_out += f\"MO_1510{day},{n_vehicles_day}\\n\"\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# writing results in S3\n",
    "s3.put_object(Body=bytes(csv_out,\"utf-8\"), Bucket='mobility-traces-sp', Key='metrics-calculation/using-new-map-matching-filter/actives-buses/active-buses-per-day-one-hour-filter.csv')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total vehicles per day - 15min count 2 or 1\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import boto3\n",
    "\n",
    "csv_out = \"day,number_of_vehicles\\n\"\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-15-min-filter-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    n_vehicles_day = traces.select(F.countDistinct(\"id_avl\").alias(\"count\")).collect()[0][\"count\"]\n",
    "    \n",
    "    csv_out += f\"MO_1510{day},{n_vehicles_day}\\n\"\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# writing results in S3\n",
    "s3.put_object(Body=bytes(csv_out,\"utf-8\"), Bucket='mobility-traces-sp', Key='metrics-calculation/using-new-map-matching-filter/actives-buses/active-buses-per-day-one-15-min-count-2-or-1.csv')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bf0ae556d34a5b8e15844f82922a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Number of vehicles per hour per day - 1 hour\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-one-hour-filter/MO_1510\"+str(day)+\"/\")\n",
    "    df = traces.groupby(\"hour_avl\").agg(F.countDistinct(\"id_avl\").alias(\"number_buses\"))\n",
    "    df.write.parquet(f\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/actives-buses/active-buses-per-hour-one-hour-filter/MO_1510{day}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dae903a429a431e902d63a843df68ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Number of vehicles per hour per day - 15min count 2 or 1\n",
    "\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-15-min-filter-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    df = traces.groupby(\"hour_avl\").agg(F.countDistinct(\"id_avl\").alias(\"number_buses\"))\n",
    "    df.write.parquet(f\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/actives-buses/active-buses-per-hour-15-min-filter-2-or-1/MO_1510{day}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d3734180df402db8b80bdb62baf3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Number of vehicles per hour and region per day - 1 Hour\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-one-hour-filter/MO_1510\"+str(day)+\"/\")\n",
    "    df = traces.groupby(\"hour_avl\",\"region\").agg(F.countDistinct(\"id_avl\").alias(\"number_buses\"))\n",
    "    df.write.parquet(f\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/actives-buses/active-buses-per-hour-per-region-one-hour-filter/MO_1510{day}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fbb68d61314e9c8a65518a37e9bbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Number of vehicles per hour and region per day - 15 minute count 2 or 1\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-15-min-filter-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    df = traces.groupby(\"hour_avl\",\"region\").agg(F.countDistinct(\"id_avl\").alias(\"number_buses\"))\n",
    "    df.write.parquet(f\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/actives-buses/active-buses-per-hour-per-region-15-min-filter-count-2-or-1/MO_1510{day}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e043fc1d7c48ee814b4f4b49ea5b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# calculating speed per vehicle - 1 Hour\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-one-hour-filter/MO_1510\"+str(day)+\"/\")\n",
    "    df_speed = traces.groupby(\"id_avl\",\"line_id\").agg(F.avg(\"speed\").alias(\"avg_speed\"),F.stddev(\"speed\").alias(\"speed_stddev\"))\n",
    "    df_speed.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/speed-calculation/speed-per-vehicle-one-hour-filter/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89347b7471b04387894117c1c574b511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# calculating speed per vehicle - 15 minute count 1 or 2\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-15-min-filter-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    df_speed = traces.groupby(\"id_avl\",\"line_id\").agg(F.avg(\"speed\").alias(\"avg_speed\"),F.stddev(\"speed\").alias(\"speed_stddev\"))\n",
    "    df_speed.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/speed-calculation/speed-per-vehicle-15-min-filter-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ae0e00190a48e5926db917c8d9104e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating speed by hour per day - 1 Hour\n",
    "\n",
    "# 1- Calculating the avg speed per bus per hour --> groupby id_avl,line_id,hour\n",
    "# 2- Calculating the avg speed per hour per day --> groupby hour\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# calculating speed per hour per day\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    # Reading traces\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-one-hour-filter/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    # Calculating speed by vehicle per hour\n",
    "    df_speed_hour_per_vehicle = traces.groupby(\"id_avl\",\"line_id\",\"hour_avl\").agg(F.avg(\"speed\").alias(\"avg_speed\"),F.stddev(\"speed\").alias(\"speed_stddev\"))\n",
    "    df_speed_hour_per_vehicle.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/speed-calculation/speed-per-vehicle-per-hour-one-hour-filter/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    # Calculating speed by hour per day\n",
    "    df_speed_hour_day = df_speed_hour_per_vehicle.groupby(\"hour_avl\").agg(F.avg(\"avg_speed\").alias(\"avg_speed\"),F.stddev(\"avg_speed\").alias(\"speed_stddev\"))\n",
    "    df_speed_hour_day.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/speed-calculation/speed-per-hour-per-day-one-hour-filter/MO_1510\"+str(day)+\"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcc69788347417aafc679b299a95033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating speed by hour per day - 15 min count 2 or 1\n",
    "\n",
    "# 1- Calculating the avg speed per bus per hour --> groupby id_avl,line_id,hour\n",
    "# 2- Calculating the avg speed per hour per day --> groupby hour\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# calculating speed per hour per day\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    # Reading traces\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-15-min-filter-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    # Calculating speed by vehicle per hour\n",
    "    df_speed_hour_per_vehicle = traces.groupby(\"id_avl\",\"line_id\",\"hour_avl\").agg(F.avg(\"speed\").alias(\"avg_speed\"),F.stddev(\"speed\").alias(\"speed_stddev\"))\n",
    "    df_speed_hour_per_vehicle.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/speed-calculation/speed-per-vehicle-per-hour-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    # Calculating speed by hour per day\n",
    "    df_speed_hour_day = df_speed_hour_per_vehicle.groupby(\"hour_avl\").agg(F.avg(\"avg_speed\").alias(\"avg_speed\"),F.stddev(\"avg_speed\").alias(\"speed_stddev\"))\n",
    "    df_speed_hour_day.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/speed-calculation/speed-per-hour-per-day-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5a339f7b6c40e0b7afc58e7698f63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating speed by region per day - 1hour\n",
    "# 1- Calculating the avg speed per bus per region --> groupby id_avl,line_id,region\n",
    "# 2- Calculating the avg speed per region per day --> groupby region\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# calculating speed per region per day\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    # Reading traces\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-one-hour-filter/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    # Calculating speed by vehicle per region\n",
    "    df_speed_region_per_vehicle = traces.groupby(\"id_avl\",\"line_id\",\"region\").agg(F.avg(\"speed\").alias(\"avg_speed\"),F.stddev(\"speed\").alias(\"speed_stddev\"))\n",
    "    df_speed_region_per_vehicle.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/speed-calculation/speed-per-vehicle-per-region-one-hour-filter/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    # Calculating speed by region per day\n",
    "    df_speed_region_day = df_speed_region_per_vehicle.groupby(\"region\").agg(F.avg(\"avg_speed\").alias(\"avg_speed\"),F.stddev(\"avg_speed\").alias(\"speed_stddev\"))\n",
    "    df_speed_region_day.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/speed-calculation/speed-per-region-per-day-one-hour-filter/MO_1510\"+str(day)+\"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3aeeccef0545a88f63baefbb0976c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating speed by region per day - 15 min  count 2 or 1\n",
    "# 1- Calculating the avg speed per bus per region --> groupby id_avl,line_id,region\n",
    "# 2- Calculating the avg speed per region per day --> groupby region\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# calculating speed per region per day\n",
    "days_to_analyze = [1,4,5,12,17,20]\n",
    "\n",
    "for day in days_to_analyze:\n",
    "    # Reading traces\n",
    "    traces = spark.read.parquet(\"s3://mobility-traces-sp/processed-data-avl-date/using-new-map-matching-filter/11-speed-calculation-filtered-15-min-filter-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    # Calculating speed by vehicle per region\n",
    "    df_speed_region_per_vehicle = traces.groupby(\"id_avl\",\"line_id\",\"region\").agg(F.avg(\"speed\").alias(\"avg_speed\"),F.stddev(\"speed\").alias(\"speed_stddev\"))\n",
    "    df_speed_region_per_vehicle.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/speed-calculation/speed-per-vehicle-per-region-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")\n",
    "    \n",
    "    # Calculating speed by region per day\n",
    "    df_speed_region_day = df_speed_region_per_vehicle.groupby(\"region\").agg(F.avg(\"avg_speed\").alias(\"avg_speed\"),F.stddev(\"avg_speed\").alias(\"speed_stddev\"))\n",
    "    df_speed_region_day.write.parquet(\"s3://mobility-traces-sp/metrics-calculation/using-new-map-matching-filter/speed-calculation/speed-per-region-per-day-15-min-count-2-or-1/MO_1510\"+str(day)+\"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
